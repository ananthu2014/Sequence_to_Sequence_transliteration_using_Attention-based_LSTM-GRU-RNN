{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabfad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import random\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3654d4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a4bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = '/home/agcl/Downloads/CS6910_ASSIGNMENT_3-without attention-sweeps.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3702b1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mananthu2014\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0da7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function to load the data'''\n",
    "def load_data(path,language_names):\n",
    "    df=pd.read_csv(path,header=None)\n",
    "    df.columns=language_names\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75791f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51200, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>transliteration_in_hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shastragaar</td>\n",
       "      <td>शस्त्रागार</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bindhya</td>\n",
       "      <td>बिन्द्या</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kirankant</td>\n",
       "      <td>किरणकांत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yagyopaveet</td>\n",
       "      <td>यज्ञोपवीत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ratania</td>\n",
       "      <td>रटानिया</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51195</th>\n",
       "      <td>toned</td>\n",
       "      <td>टोंड</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51196</th>\n",
       "      <td>mutanaazaa</td>\n",
       "      <td>मुतनाज़ा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51197</th>\n",
       "      <td>asahmaton</td>\n",
       "      <td>असहमतों</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51198</th>\n",
       "      <td>sulgaayin</td>\n",
       "      <td>सुलगायीं</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51199</th>\n",
       "      <td>anchuthengu</td>\n",
       "      <td>अंचुतेंगु</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           English transliteration_in_hindi\n",
       "0      shastragaar               शस्त्रागार\n",
       "1          bindhya                 बिन्द्या\n",
       "2        kirankant                 किरणकांत\n",
       "3      yagyopaveet                यज्ञोपवीत\n",
       "4          ratania                  रटानिया\n",
       "...            ...                      ...\n",
       "51195        toned                     टोंड\n",
       "51196   mutanaazaa                 मुतनाज़ा\n",
       "51197    asahmaton                  असहमतों\n",
       "51198    sulgaayin                 सुलगायीं\n",
       "51199  anchuthengu                अंचुतेंगु\n",
       "\n",
       "[51200 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Here,basically,the input is given to the encoder in English language and is transliterated to Hindi\n",
    "by the decoder'''\n",
    "path_train=\"/home/agcl/Downloads/hin_train.csv\"\n",
    "language_names = ['English','transliteration_in_hindi']\n",
    "df_train=load_data(path_train,language_names)\n",
    "print(df_train.shape)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b822674e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 2)\n",
      "(4096, 2)\n"
     ]
    }
   ],
   "source": [
    "#path_test=\"C:/Users/anant/Downloads/hin_test.csv\"\n",
    "path_test=\"/home/agcl/Downloads/hin_test.csv\"\n",
    "#path_validation=\"C:/Users/anant/Downloads/hin_valid.csv\"\n",
    "path_validation=\"/home/agcl/Downloads/hin_valid.csv\"\n",
    "df_validation=load_data(path_validation,language_names)\n",
    "print(df_validation.shape)\n",
    "df_test=load_data(path_test,language_names)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f13c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function for acquiring all the characters of the given data'''\n",
    "def split_words(x):\n",
    "    x=np.array(x)\n",
    "    alpha=['_','\\t','\\n',' '] #pad token, start of word, end of word and unknown tokens\n",
    "    b=[]\n",
    "    for i in range(x.shape[0]):\n",
    "        a=list(x[i])\n",
    "        for j in range(len(a)):\n",
    "            if a[j] not in b:\n",
    "                b.append(a[j])\n",
    "    b=sorted(b)\n",
    "    alpha=alpha+b\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09d5c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''All the english characters are stored into the list english_vocab and all the hindi characters are\n",
    "stored into the list hindi_vocab'''\n",
    "english_vocab=split_words(df_train['English'])\n",
    "hindi_vocab=split_words(df_train['transliteration_in_hindi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "766389b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "68\n",
      "['_', '\\t', '\\n', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['_', '\\t', '\\n', ' ', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'ळ', 'व', 'श', 'ष', 'स', 'ह', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्']\n"
     ]
    }
   ],
   "source": [
    "print(len(english_vocab))\n",
    "print(len(hindi_vocab))\n",
    "print(english_vocab)\n",
    "print(hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "209154f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Functions to create the vocabulary dictionaries with their indices'''\n",
    "def int_to_char(vocab):\n",
    "    int2char={} #padding token, start of word, end of word token and unknown token\n",
    "    for i in range(len(vocab)):\n",
    "        int2char[i]=vocab[i][0]\n",
    "    return int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49dc0e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '_', 1: '\\t', 2: '\\n', 3: ' ', 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 't', 24: 'u', 25: 'v', 26: 'w', 27: 'x', 28: 'y', 29: 'z'}\n"
     ]
    }
   ],
   "source": [
    "int2char_eng=int_to_char(english_vocab)\n",
    "print(int2char_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a0b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_int(int2char):\n",
    "    char2int={ch:ii for ii,ch in int2char.items()}\n",
    "    return char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6336f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_': 0, '\\t': 1, '\\n': 2, ' ': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29}\n"
     ]
    }
   ],
   "source": [
    "char2int_eng=char_to_int(int2char_eng)\n",
    "print(char2int_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5edad018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '_', 1: '\\t', 2: '\\n', 3: ' ', 4: 'ँ', 5: 'ं', 6: 'ः', 7: 'अ', 8: 'आ', 9: 'इ', 10: 'ई', 11: 'उ', 12: 'ऊ', 13: 'ऋ', 14: 'ए', 15: 'ऐ', 16: 'ऑ', 17: 'ओ', 18: 'औ', 19: 'क', 20: 'ख', 21: 'ग', 22: 'घ', 23: 'ङ', 24: 'च', 25: 'छ', 26: 'ज', 27: 'झ', 28: 'ञ', 29: 'ट', 30: 'ठ', 31: 'ड', 32: 'ढ', 33: 'ण', 34: 'त', 35: 'थ', 36: 'द', 37: 'ध', 38: 'न', 39: 'प', 40: 'फ', 41: 'ब', 42: 'भ', 43: 'म', 44: 'य', 45: 'र', 46: 'ल', 47: 'ळ', 48: 'व', 49: 'श', 50: 'ष', 51: 'स', 52: 'ह', 53: '़', 54: 'ऽ', 55: 'ा', 56: 'ि', 57: 'ी', 58: 'ु', 59: 'ू', 60: 'ृ', 61: 'ॅ', 62: 'े', 63: 'ै', 64: 'ॉ', 65: 'ो', 66: 'ौ', 67: '्'}\n"
     ]
    }
   ],
   "source": [
    "int2char_hin=int_to_char(hindi_vocab)\n",
    "print(int2char_hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57c7a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_': 0, '\\t': 1, '\\n': 2, ' ': 3, 'ँ': 4, 'ं': 5, 'ः': 6, 'अ': 7, 'आ': 8, 'इ': 9, 'ई': 10, 'उ': 11, 'ऊ': 12, 'ऋ': 13, 'ए': 14, 'ऐ': 15, 'ऑ': 16, 'ओ': 17, 'औ': 18, 'क': 19, 'ख': 20, 'ग': 21, 'घ': 22, 'ङ': 23, 'च': 24, 'छ': 25, 'ज': 26, 'झ': 27, 'ञ': 28, 'ट': 29, 'ठ': 30, 'ड': 31, 'ढ': 32, 'ण': 33, 'त': 34, 'थ': 35, 'द': 36, 'ध': 37, 'न': 38, 'प': 39, 'फ': 40, 'ब': 41, 'भ': 42, 'म': 43, 'य': 44, 'र': 45, 'ल': 46, 'ळ': 47, 'व': 48, 'श': 49, 'ष': 50, 'स': 51, 'ह': 52, '़': 53, 'ऽ': 54, 'ा': 55, 'ि': 56, 'ी': 57, 'ु': 58, 'ू': 59, 'ृ': 60, 'ॅ': 61, 'े': 62, 'ै': 63, 'ॉ': 64, 'ो': 65, 'ौ': 66, '्': 67}\n"
     ]
    }
   ],
   "source": [
    "char2int_hin=char_to_int(int2char_hin)\n",
    "print(char2int_hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72c0f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Finding the maximum sequence length'''\n",
    "length_eng=[len(i) for i in df_train['English']]\n",
    "length_hin=[len(i) for i in df_train['transliteration_in_hindi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "569c1309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sequence length of English words is 26\n",
      "The maximum sequence length of transliterated words is 22\n"
     ]
    }
   ],
   "source": [
    "length_eng_max=max(length_eng)+2 #we have to account for the start and end token\n",
    "print(f'The maximum sequence length of English words is {length_eng_max}')\n",
    "length_hin_max=max(length_hin)+2\n",
    "print(f'The maximum sequence length of transliterated words is {length_hin_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00064a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "num_english_tokens=len(english_vocab)\n",
    "print(num_english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e3fabb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "num_hindi_tokens=len(hindi_vocab)\n",
    "print(num_hindi_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddc02bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>transliteration_in_hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shastragaar</td>\n",
       "      <td>शस्त्रागार</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bindhya</td>\n",
       "      <td>बिन्द्या</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kirankant</td>\n",
       "      <td>किरणकांत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yagyopaveet</td>\n",
       "      <td>यज्ञोपवीत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ratania</td>\n",
       "      <td>रटानिया</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51195</th>\n",
       "      <td>toned</td>\n",
       "      <td>टोंड</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51196</th>\n",
       "      <td>mutanaazaa</td>\n",
       "      <td>मुतनाज़ा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51197</th>\n",
       "      <td>asahmaton</td>\n",
       "      <td>असहमतों</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51198</th>\n",
       "      <td>sulgaayin</td>\n",
       "      <td>सुलगायीं</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51199</th>\n",
       "      <td>anchuthengu</td>\n",
       "      <td>अंचुतेंगु</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           English transliteration_in_hindi\n",
       "0      shastragaar               शस्त्रागार\n",
       "1          bindhya                 बिन्द्या\n",
       "2        kirankant                 किरणकांत\n",
       "3      yagyopaveet                यज्ञोपवीत\n",
       "4          ratania                  रटानिया\n",
       "...            ...                      ...\n",
       "51195        toned                     टोंड\n",
       "51196   mutanaazaa                 मुतनाज़ा\n",
       "51197    asahmaton                  असहमतों\n",
       "51198    sulgaayin                 सुलगायीं\n",
       "51199  anchuthengu                अंचुतेंगु\n",
       "\n",
       "[51200 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29cd6595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(length_eng_max)\n",
    "print(length_hin_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d56f79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '_', 1: '\\t', 2: '\\n', 3: ' ', 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 't', 24: 'u', 25: 'v', 26: 'w', 27: 'x', 28: 'y', 29: 'z'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The position of padding is zero itself, so we can create tensors using torch.zeros and proceed'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(int2char_eng)\n",
    "'''The position of padding is zero itself, so we can create tensors using torch.zeros and proceed'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dfa059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df,english_vocab=english_vocab,hindi_vocab=hindi_vocab,\n",
    "                 length_eng_max=length_eng_max,length_hin_max=length_hin_max,char2int_eng=char2int_eng\n",
    "                 ,char2int_hin=char2int_hin):\n",
    "    \n",
    "    '''removing words of length more than max length'''\n",
    "    df['English'] = df['English'].str.lower()\n",
    "    df['transliteration_in_hindi'] = df['transliteration_in_hindi'].str.lower()\n",
    "    df = df[df['English'].apply(len) <= length_eng_max-2]\n",
    "    df = df[df['transliteration_in_hindi'].apply(len) <= length_hin_max-2]\n",
    "    '''Adding start and end of word tokens'''\n",
    "    y_og = df['transliteration_in_hindi'].values\n",
    "    x_og = df['English'].values\n",
    "    x = '\\t'+x_og+'\\n'\n",
    "    y = '\\t'+y_og+'\\n'\n",
    "    y_do=y_og+'\\n'\n",
    "    unknown=3\n",
    "    pad=0\n",
    "    pad_char='_'\n",
    "    unknown_char=' '\n",
    "    start=1\n",
    "    end=2\n",
    "    \n",
    "    enc_input_data=torch.zeros(len(x),length_eng_max)\n",
    "    dec_input_data=torch.zeros(len(y),length_hin_max)\n",
    "    dec_output_data=torch.zeros(len(y),length_hin_max)\n",
    "    for i, (xx,yy) in enumerate(zip(x,y)):\n",
    "        for j,char in enumerate(xx):\n",
    "            enc_input_data[i,j]=char2int_eng[char]\n",
    "        #pad character is zero so no need of assigning it again\n",
    "        for j,char in enumerate(yy):\n",
    "            if char in hindi_vocab:\n",
    "                dec_input_data[i,j]=char2int_hin[char]\n",
    "            else:\n",
    "                dec_input_data[i,j]=char2int_hin[unknown_char]\n",
    "    \n",
    "    for i, (xx,yy) in enumerate(zip(x,y_do)):\n",
    "        for j,char in enumerate(yy):\n",
    "            if char in hindi_vocab:\n",
    "                dec_output_data[i,j]=char2int_hin[char]\n",
    "            else:\n",
    "                dec_input_data[i,j]=char2int_hin[unknown_char]\n",
    "                \n",
    "    return enc_input_data,dec_input_data,dec_output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e201b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df,english_vocab=english_vocab,hindi_vocab=hindi_vocab,\n",
    "                 length_eng_max=length_eng_max,length_hin_max=length_hin_max,char2int_eng=char2int_eng\n",
    "                 ,char2int_hin=char2int_hin):\n",
    "    \n",
    "    \n",
    "    '''removing words of length more than max length'''\n",
    "    df = df[df['English'].apply(len) <= length_eng_max-2]\n",
    "    df = df[df['transliteration_in_hindi'].apply(len) <= length_hin_max-2]\n",
    "    '''Adding start and end of word tokens'''\n",
    "    y = df['transliteration_in_hindi'].values\n",
    "    x= df['English'].values\n",
    "    x = '\\t'+x+'\\n'\n",
    "    y = '\\t'+y+'\\n'\n",
    "    \n",
    "    unknown=3\n",
    "    pad=0\n",
    "    pad_char='_'\n",
    "    unknown_char=' '\n",
    "    start=1\n",
    "    end=2\n",
    "    \n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(df['English']), length_eng_max, num_english_tokens), dtype=\"float32\")\n",
    "    decoder_input_data = np.zeros(\n",
    "    (len(df['transliteration_in_hindi']), length_hin_max, num_hindi_tokens), dtype=\"float32\")\n",
    "    decoder_output_data = np.zeros(\n",
    "    (len(df['transliteration_in_hindi']), length_hin_max, num_hindi_tokens), dtype=\"float32\")\n",
    "    pad_char='_'\n",
    "    for i , (input_text,target_text) in enumerate(zip(x,y)):\n",
    "        for t,char in enumerate(input_text):\n",
    "            encoder_input_data[i,t,char2int_eng[char]]=1\n",
    "        encoder_input_data[i,t+1:,char2int_eng[pad_char]]=1\n",
    "    \n",
    "        for t,char in enumerate(target_text):\n",
    "            if char in hindi_vocab:\n",
    "                decoder_input_data[i,t,char2int_hin[char]]=1\n",
    "            else:\n",
    "                decoder_input_data[i,t,char2int_hin[unknown_char]]=1\n",
    "        decoder_input_data[i,t+1:,char2int_hin[pad_char]]=1\n",
    "    \n",
    "        '''decoder target data is one step ahead of decoder input data by one timestep\n",
    "        and doesnot includes start token'''\n",
    "        for t,char in enumerate(target_text):\n",
    "            if t>0:\n",
    "                if char in hindi_vocab:\n",
    "                    decoder_output_data[i,t-1,char2int_hin[char]]=1\n",
    "                else:\n",
    "                    decoder_output_data[i,t-1,char2int_hin[unknown_char]]=1\n",
    "                \n",
    "        decoder_output_data[i,t:,char2int_hin[pad_char]]=1\n",
    "    \n",
    "    return torch.tensor(encoder_input_data),torch.tensor(decoder_input_data),torch.tensor(decoder_output_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e5dcd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_data,dec_input_data,dec_output_data=process_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "407cc091",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data,decoder_input_data,decoder_output_data=one_hot_encoding(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14b89fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51200, 26])\n",
      "torch.Size([51200, 22])\n"
     ]
    }
   ],
   "source": [
    "print(enc_input_data.shape)\n",
    "print(dec_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9938947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51200, 26, 30])\n",
      "torch.Size([51200, 22, 68])\n",
      "torch.Size([51200, 22, 68])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "595cf37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1., 49., 51., 67., 34., 67., 45., 55., 21., 55., 45.,  2.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "print(dec_input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f82c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49., 51., 67., 34., 67., 45., 55., 21., 55., 45.,  2.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "print(dec_output_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee105331",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_data_test,dec_input_data_test,dec_output_data_test=process_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7c2e103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4095, 26])\n",
      "torch.Size([4095, 22])\n"
     ]
    }
   ],
   "source": [
    "print(enc_input_data_test.shape)\n",
    "print(dec_input_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96cb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7e8f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_data_val,dec_input_data_val,dec_output_data_val=process_data(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddd75195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 26])\n",
      "torch.Size([4096, 22])\n"
     ]
    }
   ],
   "source": [
    "print(enc_input_data_val.shape)\n",
    "print(dec_input_data_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c50840ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data_test,decoder_input_data_test,decoder_output_data_test=one_hot_encoding(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13049fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4095, 26, 30])\n",
      "torch.Size([4095, 22, 68])\n",
      "torch.Size([4095, 22, 68])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data_test.shape)\n",
    "print(decoder_input_data_test.shape)\n",
    "print(decoder_output_data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d064dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data_val,decoder_input_data_val,decoder_output_data_val=one_hot_encoding(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8130566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 26, 30])\n",
      "torch.Size([4096, 22, 68])\n",
      "torch.Size([4096, 22, 68])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data_val.shape)\n",
    "print(decoder_input_data_val.shape)\n",
    "print(decoder_output_data_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31b80fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 49, 51,  ...,  0,  0,  0],\n",
      "        [ 1, 41, 56,  ...,  0,  0,  0],\n",
      "        [ 1, 19, 56,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 1,  7, 51,  ...,  0,  0,  0],\n",
      "        [ 1, 51, 58,  ...,  0,  0,  0],\n",
      "        [ 1,  7,  5,  ...,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(decoder_input_data,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8987d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1., 49., 51.,  ...,  0.,  0.,  0.],\n",
      "        [ 1., 41., 56.,  ...,  0.,  0.,  0.],\n",
      "        [ 1., 19., 56.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  7., 51.,  ...,  0.,  0.,  0.],\n",
      "        [ 1., 51., 58.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  7.,  5.,  ...,  0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "print(dec_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09800fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49, 51, 67,  ...,  0,  0,  0],\n",
      "        [41, 56, 38,  ...,  0,  0,  0],\n",
      "        [19, 56, 45,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 7, 51, 52,  ...,  0,  0,  0],\n",
      "        [51, 58, 46,  ...,  0,  0,  0],\n",
      "        [ 7,  5, 24,  ...,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(decoder_output_data,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e12e52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49., 51., 67.,  ...,  0.,  0.,  0.],\n",
      "        [41., 56., 38.,  ...,  0.,  0.,  0.],\n",
      "        [19., 56., 45.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 7., 51., 52.,  ...,  0.,  0.,  0.],\n",
      "        [51., 58., 46.,  ...,  0.,  0.,  0.],\n",
      "        [ 7.,  5., 24.,  ...,  0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "print(dec_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "549cb4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['transliteration_in_hindi']='\\t'+df_train['transliteration_in_hindi']+'\\n'\n",
    "df_train['English']='\\t'+df_train['English']+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30e0f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['English_seq'] = df_train['English'].apply(lambda x: [char2int_eng[char] for char in x])\n",
    "df_train['transliteration_seq'] = df_train['transliteration_in_hindi'].apply(lambda x: [char2int_hin[char] for char in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aafc2e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_': 0, '\\t': 1, '\\n': 2, ' ': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29}\n"
     ]
    }
   ],
   "source": [
    "print(char2int_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19777c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_': 0, '\\t': 1, '\\n': 2, ' ': 3, 'ँ': 4, 'ं': 5, 'ः': 6, 'अ': 7, 'आ': 8, 'इ': 9, 'ई': 10, 'उ': 11, 'ऊ': 12, 'ऋ': 13, 'ए': 14, 'ऐ': 15, 'ऑ': 16, 'ओ': 17, 'औ': 18, 'क': 19, 'ख': 20, 'ग': 21, 'घ': 22, 'ङ': 23, 'च': 24, 'छ': 25, 'ज': 26, 'झ': 27, 'ञ': 28, 'ट': 29, 'ठ': 30, 'ड': 31, 'ढ': 32, 'ण': 33, 'त': 34, 'थ': 35, 'द': 36, 'ध': 37, 'न': 38, 'प': 39, 'फ': 40, 'ब': 41, 'भ': 42, 'म': 43, 'य': 44, 'र': 45, 'ल': 46, 'ळ': 47, 'व': 48, 'श': 49, 'ष': 50, 'स': 51, 'ह': 52, '़': 53, 'ऽ': 54, 'ा': 55, 'ि': 56, 'ी': 57, 'ु': 58, 'ू': 59, 'ृ': 60, 'ॅ': 61, 'े': 62, 'ै': 63, 'ॉ': 64, 'ो': 65, 'ौ': 66, '्': 67}\n"
     ]
    }
   ],
   "source": [
    "print(char2int_hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e861d24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>transliteration_in_hindi</th>\n",
       "      <th>English_seq</th>\n",
       "      <th>transliteration_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\tshastragaar\\n</td>\n",
       "      <td>\\tशस्त्रागार\\n</td>\n",
       "      <td>[1, 22, 11, 4, 22, 23, 21, 4, 10, 4, 4, 21, 2]</td>\n",
       "      <td>[1, 49, 51, 67, 34, 67, 45, 55, 21, 55, 45, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\tbindhya\\n</td>\n",
       "      <td>\\tबिन्द्या\\n</td>\n",
       "      <td>[1, 5, 12, 17, 7, 11, 28, 4, 2]</td>\n",
       "      <td>[1, 41, 56, 38, 67, 36, 67, 44, 55, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tkirankant\\n</td>\n",
       "      <td>\\tकिरणकांत\\n</td>\n",
       "      <td>[1, 14, 12, 21, 4, 17, 14, 4, 17, 23, 2]</td>\n",
       "      <td>[1, 19, 56, 45, 33, 19, 55, 5, 34, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tyagyopaveet\\n</td>\n",
       "      <td>\\tयज्ञोपवीत\\n</td>\n",
       "      <td>[1, 28, 4, 10, 28, 18, 19, 4, 25, 8, 8, 23, 2]</td>\n",
       "      <td>[1, 44, 26, 67, 28, 65, 39, 48, 57, 34, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\tratania\\n</td>\n",
       "      <td>\\tरटानिया\\n</td>\n",
       "      <td>[1, 21, 4, 23, 4, 17, 12, 4, 2]</td>\n",
       "      <td>[1, 45, 29, 55, 38, 56, 44, 55, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51195</th>\n",
       "      <td>\\ttoned\\n</td>\n",
       "      <td>\\tटोंड\\n</td>\n",
       "      <td>[1, 23, 18, 17, 8, 7, 2]</td>\n",
       "      <td>[1, 29, 65, 5, 31, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51196</th>\n",
       "      <td>\\tmutanaazaa\\n</td>\n",
       "      <td>\\tमुतनाज़ा\\n</td>\n",
       "      <td>[1, 16, 24, 23, 4, 17, 4, 4, 29, 4, 4, 2]</td>\n",
       "      <td>[1, 43, 58, 34, 38, 55, 26, 53, 55, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51197</th>\n",
       "      <td>\\tasahmaton\\n</td>\n",
       "      <td>\\tअसहमतों\\n</td>\n",
       "      <td>[1, 4, 22, 4, 11, 16, 4, 23, 18, 17, 2]</td>\n",
       "      <td>[1, 7, 51, 52, 43, 34, 65, 5, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51198</th>\n",
       "      <td>\\tsulgaayin\\n</td>\n",
       "      <td>\\tसुलगायीं\\n</td>\n",
       "      <td>[1, 22, 24, 15, 10, 4, 4, 28, 12, 17, 2]</td>\n",
       "      <td>[1, 51, 58, 46, 21, 55, 44, 57, 5, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51199</th>\n",
       "      <td>\\tanchuthengu\\n</td>\n",
       "      <td>\\tअंचुतेंगु\\n</td>\n",
       "      <td>[1, 4, 17, 6, 11, 24, 23, 11, 8, 17, 10, 24, 2]</td>\n",
       "      <td>[1, 7, 5, 24, 58, 34, 62, 5, 21, 58, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               English transliteration_in_hindi  \\\n",
       "0      \\tshastragaar\\n           \\tशस्त्रागार\\n   \n",
       "1          \\tbindhya\\n             \\tबिन्द्या\\n   \n",
       "2        \\tkirankant\\n             \\tकिरणकांत\\n   \n",
       "3      \\tyagyopaveet\\n            \\tयज्ञोपवीत\\n   \n",
       "4          \\tratania\\n              \\tरटानिया\\n   \n",
       "...                ...                      ...   \n",
       "51195        \\ttoned\\n                 \\tटोंड\\n   \n",
       "51196   \\tmutanaazaa\\n             \\tमुतनाज़ा\\n   \n",
       "51197    \\tasahmaton\\n              \\tअसहमतों\\n   \n",
       "51198    \\tsulgaayin\\n             \\tसुलगायीं\\n   \n",
       "51199  \\tanchuthengu\\n            \\tअंचुतेंगु\\n   \n",
       "\n",
       "                                           English_seq  \\\n",
       "0       [1, 22, 11, 4, 22, 23, 21, 4, 10, 4, 4, 21, 2]   \n",
       "1                      [1, 5, 12, 17, 7, 11, 28, 4, 2]   \n",
       "2             [1, 14, 12, 21, 4, 17, 14, 4, 17, 23, 2]   \n",
       "3       [1, 28, 4, 10, 28, 18, 19, 4, 25, 8, 8, 23, 2]   \n",
       "4                      [1, 21, 4, 23, 4, 17, 12, 4, 2]   \n",
       "...                                                ...   \n",
       "51195                         [1, 23, 18, 17, 8, 7, 2]   \n",
       "51196        [1, 16, 24, 23, 4, 17, 4, 4, 29, 4, 4, 2]   \n",
       "51197          [1, 4, 22, 4, 11, 16, 4, 23, 18, 17, 2]   \n",
       "51198         [1, 22, 24, 15, 10, 4, 4, 28, 12, 17, 2]   \n",
       "51199  [1, 4, 17, 6, 11, 24, 23, 11, 8, 17, 10, 24, 2]   \n",
       "\n",
       "                                  transliteration_seq  \n",
       "0      [1, 49, 51, 67, 34, 67, 45, 55, 21, 55, 45, 2]  \n",
       "1              [1, 41, 56, 38, 67, 36, 67, 44, 55, 2]  \n",
       "2               [1, 19, 56, 45, 33, 19, 55, 5, 34, 2]  \n",
       "3          [1, 44, 26, 67, 28, 65, 39, 48, 57, 34, 2]  \n",
       "4                  [1, 45, 29, 55, 38, 56, 44, 55, 2]  \n",
       "...                                               ...  \n",
       "51195                           [1, 29, 65, 5, 31, 2]  \n",
       "51196          [1, 43, 58, 34, 38, 55, 26, 53, 55, 2]  \n",
       "51197                [1, 7, 51, 52, 43, 34, 65, 5, 2]  \n",
       "51198           [1, 51, 58, 46, 21, 55, 44, 57, 5, 2]  \n",
       "51199         [1, 7, 5, 24, 58, 34, 62, 5, 21, 58, 2]  \n",
       "\n",
       "[51200 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22627fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_data=enc_input_data.long()\n",
    "dec_input_data=dec_input_data.long()\n",
    "enc_input_data_test=enc_input_data_test.long()\n",
    "dec_input_data_test=dec_input_data_test.long()\n",
    "enc_input_data_val=enc_input_data_val.long()\n",
    "dec_input_data_val=dec_input_data_val.long()\n",
    "encoder_input_data=encoder_input_data.long()\n",
    "decoder_input_data=decoder_input_data.long()\n",
    "decoder_output_data=decoder_output_data.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0235c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_word_accuracy(dec_predicted_data, dec_output_data):\n",
    "#     #Here, we have to pass the arguments in the shape(batch_size,sequence_length)\n",
    "#     batch_size = dec_predicted_data.shape[0]\n",
    "#     dec_predicted_data=torch.argmax(dec_predicted_data,dim=-1)\n",
    "#     dec_output_data=torch.argmax(dec_output_data,dim=-1)\n",
    "#     with torch.no_grad():\n",
    "#         true_words = 0\n",
    "#         for i in range(batch_size):\n",
    "#             mark = True\n",
    "#             for j in range(dec_predicted_data.shape[1]):\n",
    "#                 if dec_predicted_data[i, j] == dec_output_data[i, j]:\n",
    "#                     mark = True\n",
    "#                 else:\n",
    "#                     mark = False\n",
    "#                     break\n",
    "#             if mark == True:\n",
    "#                 true_words =true_words+ 1\n",
    "#     return (true_words / batch_size)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d756511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_accuracy(dec_predicted_data, dec_output_data):\n",
    "    # Here, we have to pass the arguments in the shape (batch_size, sequence_length)\n",
    "    dec_predicted_data = torch.argmax(dec_predicted_data, dim=-1)\n",
    "    dec_output_data = torch.argmax(dec_output_data, dim=-1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        match = (dec_predicted_data == dec_output_data).all(dim=1)\n",
    "        true_words = match.sum().item()\n",
    "        batch_size = dec_predicted_data.shape[0]\n",
    "    \n",
    "    accuracy = (true_words / batch_size) * 100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e1a52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_acc=calculate_word_accuracy(decoder_input_data,decoder_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd3abf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(word_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1428344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_char_accuracy(decoder_predicted_data, decoder_output_data):\n",
    "    #Here, we have to pass the arguments in the shape(batch_size,sequence_length,unique_tokens)\n",
    "    batch_size, seq_length,unique_tokens = decoder_predicted_data.shape\n",
    "    dec_predicted_data=torch.argmax(decoder_predicted_data,dim=-1)\n",
    "    dec_output_data=torch.argmax(decoder_output_data,dim=-1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct_count = (dec_predicted_data == dec_output_data).sum().item()\n",
    "        return (correct_count / (seq_length * batch_size))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01bdebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_accuracy = calculate_char_accuracy(decoder_input_data[7:9,:,:],decoder_input_data[1:3,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a66732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.36363636363637\n"
     ]
    }
   ],
   "source": [
    "print(char_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1e9e3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ea2b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens_eng=len(english_vocab)\n",
    "unique_tokens_hin=len(hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e331c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1287bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(x,y,z,batch_size,device=device):\n",
    "    \n",
    "    x=x.to(device)\n",
    "    y=y.to(device)\n",
    "    z=z.to(device)\n",
    "    combined=TensorDataset(x,y,z)\n",
    "    loader=DataLoader(combined,batch_size=batch_size,shuffle=False,drop_last=True)#required in test data\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07817af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, layer_dim, drop_out, bi_dir, cell,unique_tokens_eng):\n",
    "        '''unique_token_hin is the third dimension in one hot encoding or no of tokens in eng or input size'''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.cell = cell\n",
    "        self.bi_dir = bi_dir\n",
    "        self.unique_tokens_eng=unique_tokens_eng\n",
    "        self.embed_dim=embed_dim\n",
    "        self.drop_out=drop_out\n",
    "        '''The input to the encoder will be of shape (batch_size,sequence_length) and the output size will be\n",
    "        (batch_size,seq_length,embed_size)'''\n",
    "        self.drop__out=nn.Dropout(p=self.drop_out)\n",
    "\n",
    "        self.embedding = nn.Embedding(unique_tokens_eng, embed_dim) \n",
    "        self.relu=nn.ReLU()\n",
    "        self.rnn=nn.RNN(embed_dim,hidden_dim,dropout=self.drop_out,\n",
    "                        num_layers=layer_dim,bidirectional=bi_dir,batch_first=True)\n",
    "        self.gru=nn.GRU(embed_dim,hidden_dim,dropout=self.drop_out,\n",
    "                        num_layers=layer_dim,bidirectional=bi_dir,batch_first=True)\n",
    "        self.lstm=nn.LSTM(embed_dim,hidden_dim,dropout=self.drop_out,\n",
    "                         num_layers=layer_dim,bidirectional=bi_dir,batch_first=True)\n",
    "        self.linear =nn.Linear(self.hidden_dim*(1+int(self.bi_dir)),self.hidden_dim)\n",
    "\n",
    "    def forward(self, x,hidden):\n",
    "        '''Here, x is the encoder input data'''\n",
    "        batch_size=x.size(0)\n",
    "        embedding_input = self.embedding(x)\n",
    "        embedding_input = self.drop__out(embedding_input)\n",
    "        embedding_input = self.relu(embedding_input)\n",
    "        embedding_input=embedding_input.to(device)\n",
    "        if self.cell==\"GRU\":\n",
    "            output,h_n=self.gru(embedding_input,hidden)\n",
    "            return output,h_n\n",
    "        elif self.cell=='RNN':\n",
    "            output,h_n=self.gru(embedding_input,hidden)\n",
    "            return output,h_n\n",
    "        elif self.cell=='LSTM':\n",
    "            output,(h_n,c_n)=self.lstm(embedding_input,hidden)\n",
    "            return output,(h_n,c_n)\n",
    "        \n",
    "    def encoder_initial(self,batch_size,device=device):\n",
    "        if self.cell == \"LSTM\":\n",
    "            h_0 =torch.randn((1 + int(self.bi_dir)) * self.layer_dim, batch_size, self.hidden_dim, device=device)\n",
    "            c_0 =torch.randn((1 + int(self.bi_dir)) * self.layer_dim, batch_size, self.hidden_dim, device=device) \n",
    "            return (h_0,c_0)\n",
    "        #H_0,C_0 HAVE SAME DIMENSION\n",
    "        else:\n",
    "            h_0=torch.randn((1 + int(self.bi_dir)) * self.layer_dim, batch_size, self.hidden_dim, device=device)\n",
    "            return h_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f55ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, layer_dim, drop_out, bi_dir, cell,unique_tokens_hin):\n",
    "        '''unique_token_hin is the third dimension in one hot encoding or no of tokens in hindi or output size'''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.cell = cell\n",
    "        self.bi_dir = bi_dir\n",
    "        self.unique_tokens_hin=unique_tokens_hin\n",
    "        self.embed_dim=embed_dim\n",
    "        self.drop_out=drop_out\n",
    "        '''The input to the embedding will be of shape (batch_size,sequence_length) and the output size will be\n",
    "        (batch_size,seq_length,embed_size)'''\n",
    "\n",
    "        self.embedding = nn.Embedding(unique_tokens_hin, embed_dim) \n",
    "        self.drop__out=nn.Dropout(p=self.drop_out)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.rnn=nn.RNN(embed_dim,hidden_dim,dropout=self.drop_out,\n",
    "                        num_layers=layer_dim,bidirectional=bi_dir,batch_first=True)\n",
    "        self.gru=nn.GRU(embed_dim,hidden_dim,dropout=self.drop_out,\n",
    "                        num_layers=layer_dim,bidirectional=bi_dir,batch_first=True)\n",
    "        self.lstm=nn.LSTM(embed_dim,hidden_dim,dropout=self.drop_out,\n",
    "                         num_layers=layer_dim,bidirectional=bi_dir,batch_first=True)\n",
    "        self.out_put = nn.Linear((1 + int(self.bi_dir)) * self.hidden_dim, self.unique_tokens_hin)\n",
    "        #number of unique tokens in hindi is the output dimension of decoder layer\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x,hidden):\n",
    "        '''Here, x is the decoder input data'''\n",
    "        batch_size=x.size(0)\n",
    "        embedding_output = self.embedding(x.long())\n",
    "        embedding_output=embedding_output.to(device)\n",
    "        #embedding_output = self.drop__out(embedding_output)\n",
    "        if self.cell==\"GRU\":\n",
    "            hidden=hidden.contiguous()\n",
    "            output,h_n=self.gru(embedding_output,hidden)\n",
    "            output=self.relu(output)\n",
    "            output=self.softmax(self.out_put(output))\n",
    "            return output,h_n\n",
    "        elif self.cell=='RNN':\n",
    "            output,h_n=self.gru(embedding_output,hidden)\n",
    "            output=self.relu(output)\n",
    "            output=self.softmax(self.out_put(output))\n",
    "            return output,h_n\n",
    "        elif self.cell=='LSTM':\n",
    "            h0=hidden[0].contiguous()\n",
    "            c0=hidden[1].contiguous()\n",
    "            output,(h_n,c_n)=self.lstm(embedding_output,(h0,c0))\n",
    "            output=self.relu(output)\n",
    "            output=self.softmax(self.out_put(output))\n",
    "            return output,(h_n,c_n)\n",
    "            \n",
    "    def decoder_initial(self,batch_size,device=device):\n",
    "        if self.cell == \"LSTM\":\n",
    "            h_0 =torch.randn((1 + int(self.bi_dir)) * self.layer_dim, batch_size, self.hidden_dim, device=device)\n",
    "            c_0=torch.randn((1 + int(self.bi_dir)) * self.layer_dim, batch_size, self.hidden_dim, device=device) \n",
    "            return (h_0,c_0)\n",
    "          #H_0,C_0 HAVE SAME DIMENSION\n",
    "        else:\n",
    "            h_0=torch.randn((1 + int(self.bi_dir)) * self.layer_dim, batch_size, self.hidden_dim, device=device)\n",
    "            return h_0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ce055ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To use when the number of encoder and decoder layers are different'''\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, num_enc_layers, num_dec_layers, cell, bi_dir):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.num_enc_layers = num_enc_layers\n",
    "        self.num_dec_layers = num_dec_layers\n",
    "        self.cell = cell\n",
    "        self.bi_dir = bi_dir\n",
    "        self.linear = nn.Linear(num_enc_layers * int(1 + bi_dir), num_dec_layers * int(1 + bi_dir))\n",
    "\n",
    "    def forward(self, h_n_enc):\n",
    "        if self.cell == 'LSTM':\n",
    "            x=x.permute(*torch.arange(x.ndim - 1, -1, -1))\n",
    "            h_dec = self.linear(h_n_enc[0].permute(*torch.arange(h_n_enc.ndim - 1, -1, -1)))\n",
    "            c_dec = self.linear(h_n_enc[1].permute(*torch.arange(c_n_enc.ndim - 1, -1, -1)))\n",
    "            h_0_dec = (h_dec.permute(*torch.arange(h_dec.ndim - 1, -1, -1)),\n",
    "                       c_dec.permute(*torch.arange(c_dec.ndim - 1, -1, -1)))\n",
    "        else:\n",
    "            h_0_dec = self.linear(h_n_enc.permute(*torch.arange(h_n_enc.ndim - 1, -1, -1)))\n",
    "            h_0_dec = h_0_dec.permute(*torch.arange(h_0_dec.ndim - 1, -1, -1))\n",
    "        return h_0_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e8fbede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(input_tensor, target_tensor,target_onehot, encoder_model, decoder_model, encoder_optimizer,\n",
    "          decoder_optimizer,hidden_dim,criterion,input_length,target_length,batch_size,\n",
    "             teacher_forcing_ratio,num_enc_layers,num_dec_layers,bi_dir,cell,reshape,ropt,device=device):\n",
    "    \n",
    "    \n",
    "    h_0_enc = encoder_model.encoder_initial(batch_size)\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    #ropt.zero_grad()\n",
    "    uh=target_onehot.shape[-1]\n",
    "    decoder_predicted=torch.zeros(batch_size,target_length,uh,device=device)\n",
    "    loss = 0\n",
    "    \n",
    "    encoder_model.train()\n",
    "    decoder_model.train()\n",
    "    #reshape.train()\n",
    "    \n",
    "    '''Encoder model is given and the representation of input word is taken from it'''\n",
    "    for i in range(input_length):\n",
    "        output_enc, h_n_enc = encoder_model(input_tensor[:,i].unsqueeze(1), h_0_enc)\n",
    "        h_0_enc=h_n_enc\n",
    "\n",
    "    dec_input = torch.ones(batch_size,1,device=device)#start token is given as the input and the indices is 1.\n",
    "    #h_0_dec=reshape(h_n_enc)\n",
    "    h_0_dec=h_n_enc\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for i in range(target_length):\n",
    "            output_dec, h_n_dec = decoder_model(dec_input, h_0_dec)\n",
    "            decoder_predicted[:, i, :] = output_dec.squeeze(1)\n",
    "            loss += criterion(output_dec.reshape(-1,uh).float(), target_onehot[:,i:i+1,:].reshape(-1,uh).float())\n",
    "            #loss+=criterion(output_dec.view(-1,uh).float(),target_tensor[:,i].long())\n",
    "            dec_input = target_tensor[:,i].unsqueeze(1)  # Teacher forcing\n",
    "            h_0_dec = h_n_dec\n",
    "            \n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for i in range(target_length):\n",
    "            output_dec, h_n_dec = decoder_model(dec_input, h_0_dec)\n",
    "            #top_values, top_indices = output_dec.topk(k=1, dim=2)\n",
    "            #dec_input = top_indices.view(-1,1).detach()# detach from history as input\n",
    "            dec_input = torch.argmax(output_dec,dim=-1)\n",
    "            decoder_predicted[:, i, :] = output_dec.squeeze(1)\n",
    "            loss += criterion(output_dec.reshape(-1,uh).float(), target_onehot[:,i:i+1,:].reshape(-1,uh).float())\n",
    "            #loss+=criterion(output_dec.view(-1,uh).float(),target_tensor[:,i].long())\n",
    "            h_0_dec=h_n_dec\n",
    "    \n",
    "            \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    #ropt.step()\n",
    "    \n",
    "    loss_batch = loss.item() / target_length\n",
    "    char_acc_batch = calculate_char_accuracy(decoder_predicted,target_onehot)\n",
    "    word_acc_batch = calculate_word_accuracy(decoder_predicted,target_onehot)\n",
    "    return loss_batch, char_acc_batch, word_acc_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "239ffb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(input_tensor, target_tensor,target_onehot, encoder_model, decoder_model,reshape,\n",
    "          hidden_dim,criterion,input_length,target_length,batch_size,\n",
    "             num_enc_layers,num_dec_layers,bi_dir,cell,device=device):\n",
    "    \n",
    "    \n",
    "    h_0_enc = encoder_model.encoder_initial(batch_size)\n",
    "    uh=target_onehot.shape[-1]\n",
    "    decoder_predicted=torch.zeros(batch_size,target_length,uh,device=device)\n",
    "    loss = 0\n",
    "    \n",
    "    encoder_model.eval()\n",
    "    decoder_model.eval()\n",
    "    #reshape.eval()\n",
    "    \n",
    "    '''Encoder model is given and the representation of input word is taken from it'''\n",
    "    for i in range(input_length):\n",
    "        output_enc, h_n_enc = encoder_model(input_tensor[:,i].unsqueeze(1), h_0_enc)\n",
    "        h_0_enc=h_n_enc\n",
    "\n",
    "    dec_input = torch.ones(batch_size,1,device=device)#start token is given as the input and the indices is 1.\n",
    "    #h_0_dec=reshape(h_n_enc)\n",
    "    h_0_dec=h_n_enc\n",
    "    \n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for i in range(target_length):\n",
    "        output_dec, h_n_dec = decoder_model(dec_input, h_0_dec)\n",
    "        #top_values, top_indices = output_dec.topk(k=1, dim=2)\n",
    "        #dec_input = top_indices.view(-1,1).detach()# detach from history as input\n",
    "        dec_input = torch.argmax(output_dec,dim=-1)\n",
    "        decoder_predicted[:, i, :] = output_dec.squeeze(1)\n",
    "        h_0_dec=h_n_dec\n",
    "        loss += criterion(output_dec.reshape(-1,uh).float(), target_onehot[:,i:i+1,:].reshape(-1,uh).float())\n",
    "        #loss+=criterion(output_dec.view(-1,uh).float(),target_tensor[:,i].long())\n",
    "    \n",
    "    loss_batch = loss.item() / target_length\n",
    "    char_acc_batch = calculate_char_accuracy(decoder_predicted,target_onehot)\n",
    "    word_acc_batch = calculate_word_accuracy(decoder_predicted,target_onehot)\n",
    "    #words=indices_to_words(torch.argmax(decoder_predicted,dim=-1))\n",
    "#     print(words)\n",
    "#     print('\\n')\n",
    "    #tgt = indices_to_words(torch.argmax(target_onehot,dim=-1))\n",
    "#     print(tgt)\n",
    "    return loss_batch, char_acc_batch, word_acc_batch,torch.argmax(decoder_predicted,dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44469cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x=enc_input_data,y=dec_output_data,yonehot=decoder_output_data,\n",
    "          x_val=enc_input_data_val,y_val=dec_output_data_val,yonehot_val=decoder_output_data_val,\n",
    "          epochs=10,optimizer='NAdam',learning_rate=0.001,weight_decay=0.0001,layer_dim=2,bi_dir=True,\n",
    "          teacher_forcing_ratio=0.5,cell='LSTM',embed_dim=128,hidden_dim=256,batch_size=64,drop_out=0.2,device=device):\n",
    "    \n",
    "    start = time.time()\n",
    "    input_length = x.shape[1]\n",
    "    target_length = y.shape[1]\n",
    "    num_enc_layers=layer_dim\n",
    "    num_dec_layers=layer_dim\n",
    "    \n",
    "    encoder_model = Encoder(embed_dim, hidden_dim, num_enc_layers, drop_out, bi_dir, cell,unique_tokens_eng).to(device)\n",
    "    decoder_model = Decoder(embed_dim, hidden_dim, num_dec_layers, drop_out, bi_dir, cell,unique_tokens_hin).to(device)\n",
    "    reshape=Reshape(num_enc_layers, num_dec_layers, cell, bi_dir).to(device)\n",
    "    \n",
    "    if optimizer=='Adam':\n",
    "        encoder_optimizer=torch.optim.Adam(encoder_model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "        decoder_optimizer=torch.optim.Adam(decoder_model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "        ropt = torch.optim.Adam(reshape.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "    elif optimizer=='NAdam':\n",
    "        encoder_optimizer=torch.optim.NAdam(encoder_model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "        decoder_optimizer=torch.optim.NAdam(decoder_model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "        ropt = torch.optim.NAdam(reshape.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "    elif optimizer=='SGD':\n",
    "        \n",
    "        encoder_optimizer=torch.optim.SGD(encoder_model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "        decoder_optimizer=torch.optim.SGD(decoder_model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "        ropt = torch.optim.SGD(reshape.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "        \n",
    "    train_loader = data_loader(x,y,yonehot,batch_size,device=device)\n",
    "    val_loader = data_loader(x_val,y_val,yonehot_val,batch_size,device=device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #criterion=nn.NLLLoss()\n",
    "    \n",
    "    epoch_losses=[]\n",
    "    epoch_char_accuracy=[]\n",
    "    epoch_word_accuracy=[]\n",
    "    epoch_losses_val=[]\n",
    "    epoch_char_accuracy_val=[]\n",
    "    epoch_word_accuracy_val=[]\n",
    "    \n",
    "    for i in range(1, epochs + 1):\n",
    "        batch_losses=[]\n",
    "        batch_char_acc=[]\n",
    "        batch_word_acc=[]\n",
    "        batch_losses_val=[]\n",
    "        batch_char_acc_val=[]\n",
    "        batch_word_acc_val=[]\n",
    "        for enc_input_tensor,dec_target_tensor,dec_onehot in train_loader:\n",
    "            \n",
    "            loss_batch,char_acc_batch,word_acc_batch = gradient(enc_input_tensor,dec_target_tensor,dec_onehot,\n",
    "                    encoder_model,decoder_model,encoder_optimizer,decoder_optimizer,\n",
    "                    hidden_dim,criterion,input_length,target_length,batch_size, teacher_forcing_ratio,\n",
    "                            num_enc_layers,num_dec_layers,bi_dir,cell,reshape,ropt,device=device)\n",
    "            \n",
    "        for enc_input_tensor_val,dec_target_tensor_val,dec_onehot_val in val_loader: \n",
    "           \n",
    "            \n",
    "            loss_batch_val,char_acc_batch_val,word_acc_batch_val,dec_pre=testing(enc_input_tensor_val,dec_target_tensor_val,\n",
    "                        dec_onehot_val,encoder_model,decoder_model,reshape,hidden_dim,criterion,input_length,\n",
    "                        target_length,batch_size,num_enc_layers,num_dec_layers,bi_dir,cell,device=device)\n",
    "          \n",
    "            batch_losses.append(loss_batch)\n",
    "            batch_char_acc.append(char_acc_batch)\n",
    "            batch_word_acc.append(word_acc_batch)\n",
    "            batch_losses_val.append(loss_batch_val)\n",
    "            batch_char_acc_val.append(char_acc_batch_val)\n",
    "            batch_word_acc_val.append(word_acc_batch_val)\n",
    "            \n",
    "            \n",
    "        epoch_loss=sum(batch_losses)/len(batch_losses)\n",
    "        epoch_char_acc=sum(batch_char_acc)/len(batch_char_acc)\n",
    "        epoch_word_acc=sum(batch_word_acc)/len(batch_word_acc)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        epoch_char_accuracy.append(epoch_char_acc)\n",
    "        epoch_word_accuracy.append(epoch_word_acc)\n",
    "        \n",
    "        epoch_loss_val = sum(batch_losses_val) / len(batch_losses_val)\n",
    "        epoch_char_acc_val = sum(batch_char_acc_val) / len(batch_char_acc_val)\n",
    "        epoch_word_acc_val = sum(batch_word_acc_val) / len(batch_word_acc_val)\n",
    "        epoch_losses_val.append(epoch_loss_val)\n",
    "        epoch_char_accuracy_val.append(epoch_char_acc_val)\n",
    "        epoch_word_accuracy_val.append(epoch_word_acc_val)\n",
    "      \n",
    "        #wandb.log({'train_loss': epoch_loss, 'train_char_acc': epoch_char_acc, 'train_word_acc': epoch_word_acc, 'valid_loss': epoch_loss_val, 'valid_char_acc': epoch_char_acc_val, 'valid_word_acc': epoch_word_acc_val})\n",
    "        print(f'{timeSince(start, i / epochs)} ({i} {i / epochs * 100:.2f}%) Trainloss: {epoch_losses[-1]:.4f} Char Accuracy: {epoch_char_accuracy[-1]:.4f} Word Accuracy: {epoch_word_accuracy[-1]:.4f}')\n",
    "        print(f'{timeSince(start, i / epochs)} ({i} {i / epochs * 100:.2f}%) Validationloss: {epoch_losses_val[-1]:.4f} Char Accuracy: {epoch_char_accuracy_val[-1]:.4f} Word Accuracy: {epoch_word_accuracy_val[-1]:.4f}')\n",
    "\n",
    "\n",
    "    return encoder_model,decoder_model,reshape,encoder_optimizer,decoder_optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e693bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_acc_model(encoder_model,decoder_model,reshape,encoder_optimizer,decoder_optimizer,x=enc_input_data_test,\n",
    "                   y=dec_output_data_test,yonehot=decoder_output_data_test,epochs=10,layer_dim=2,bi_dir=True,cell='LSTM',\n",
    "                   embed_dim=128,hidden_dim=256,batch_size=4095,drop_out=0.2,device=device):\n",
    "    \n",
    "    start = time.time()\n",
    "    input_length = x.shape[1]\n",
    "    target_length = y.shape[1]\n",
    "    num_enc_layers=layer_dim\n",
    "    num_dec_layers=layer_dim\n",
    "    num_batches = x.shape[0]//batch_size\n",
    "    x = x[:num_batches*batch_size,:]\n",
    "    y = y[:num_batches*batch_size,:]\n",
    "    yonehot = yonehot[:num_batches*batch_size,:,:]\n",
    "    \n",
    "        \n",
    "    test_loader = data_loader(x,y,yonehot,batch_size,device=device)\n",
    "    decoder_predicted = torch.zeros(x.shape[0],target_length,device=device)\n",
    "\n",
    "    epoch_losses_test=[]\n",
    "    epoch_char_accuracy_test=[]\n",
    "    epoch_word_accuracy_test=[]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "       \n",
    "    for i,(enc_input_tensor_test,dec_target_tensor_test,dec_onehot_test) in enumerate(test_loader): \n",
    "\n",
    "        batch_losses_test=[]\n",
    "        batch_char_acc_test=[]\n",
    "        batch_word_acc_test=[]\n",
    "           \n",
    "        loss_batch_test,char_acc_batch_test,word_acc_batch_test,dec_tgt_batch = testing(enc_input_tensor_test,\n",
    "                        dec_target_tensor_test,dec_onehot_test,encoder_model,decoder_model,reshape,\n",
    "                        hidden_dim,criterion,input_length,target_length,batch_size,num_enc_layers,\n",
    "                                                            num_dec_layers,bi_dir,cell,device=device)\n",
    "\n",
    "        decoder_predicted[i*batch_size:(i+1)*batch_size,:] = dec_tgt_batch\n",
    "        batch_losses_test.append(loss_batch_test)\n",
    "        batch_char_acc_test.append(char_acc_batch_test)\n",
    "        batch_word_acc_test.append(word_acc_batch_test)\n",
    "            \n",
    "            \n",
    "        epoch_loss_test=sum(batch_losses_test)/len(batch_losses_test)\n",
    "        epoch_char_acc_test=sum(batch_char_acc_test)/len(batch_char_acc_test)\n",
    "        epoch_word_acc_test=sum(batch_word_acc_test)/len(batch_word_acc_test)\n",
    "        epoch_losses_test.append(epoch_loss_test)\n",
    "        epoch_char_accuracy_test.append(epoch_char_acc_test)\n",
    "        epoch_word_accuracy_test.append(epoch_word_acc_test)\n",
    "        #tgt = indices_to_words(decoder_predicted)\n",
    "\n",
    "    #print(f'{timeSince(start, i / epochs)} ({i} {i / epochs * 100:.2f}%) Testloss: {epoch_losses_test[-1]:.4f} Char Accuracy: {epoch_char_accuracy_test[-1]:.4f} Word Accuracy: {epoch_word_accuracy_test[-1]:.4f}')\n",
    "        \n",
    "\n",
    "\n",
    "    return epoch_losses_test,epoch_char_accuracy_test,epoch_word_accuracy_test,decoder_predicted\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35a40a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The hyperparameters of best accuracy model is given below:-'''\n",
    "epochs=20\n",
    "learning_rate=0.01\n",
    "hidden_dim=256\n",
    "layer_dim=2\n",
    "embed_dim=256\n",
    "cell='LSTM'\n",
    "bi_dir=True\n",
    "weight_decay=0.001\n",
    "optimizer='NAdam'\n",
    "loss='Crossentropy'\n",
    "drop_out = 0.3\n",
    "teacher_forcing_ratio = 0.5\n",
    "batch_size=64\n",
    "x=enc_input_data\n",
    "y=dec_output_data\n",
    "yonehot=decoder_output_data\n",
    "x_val=enc_input_data_val\n",
    "y_val=dec_output_data_val\n",
    "yonehot_val=decoder_output_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31ce1b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 34s (- 5m 8s) (1 10.00%) Trainloss: 0.9393 Char Accuracy: 72.7273 Word Accuracy: 0.0000\n",
      "0m 34s (- 5m 8s) (1 10.00%) Validationloss: 1.0417 Char Accuracy: 72.0170 Word Accuracy: 0.0000\n",
      "1m 8s (- 4m 34s) (2 20.00%) Trainloss: 0.8130 Char Accuracy: 75.5682 Word Accuracy: 4.6875\n",
      "1m 8s (- 4m 34s) (2 20.00%) Validationloss: 0.5910 Char Accuracy: 82.1456 Word Accuracy: 12.9883\n",
      "1m 43s (- 4m 2s) (3 30.00%) Trainloss: 0.3021 Char Accuracy: 91.2642 Word Accuracy: 14.0625\n",
      "1m 43s (- 4m 2s) (3 30.00%) Validationloss: 0.4966 Char Accuracy: 84.8566 Word Accuracy: 22.4365\n",
      "2m 18s (- 3m 27s) (4 40.00%) Trainloss: 0.5515 Char Accuracy: 82.3864 Word Accuracy: 14.0625\n",
      "2m 18s (- 3m 27s) (4 40.00%) Validationloss: 0.4443 Char Accuracy: 86.0052 Word Accuracy: 27.0752\n",
      "2m 52s (- 2m 52s) (5 50.00%) Trainloss: 0.2291 Char Accuracy: 93.4659 Word Accuracy: 29.6875\n",
      "2m 52s (- 2m 52s) (5 50.00%) Validationloss: 0.4186 Char Accuracy: 86.9940 Word Accuracy: 30.6885\n",
      "3m 27s (- 2m 18s) (6 60.00%) Trainloss: 0.2137 Char Accuracy: 94.6733 Word Accuracy: 31.2500\n",
      "3m 27s (- 2m 18s) (6 60.00%) Validationloss: 0.3950 Char Accuracy: 87.4623 Word Accuracy: 32.3486\n",
      "4m 1s (- 1m 43s) (7 70.00%) Trainloss: 0.1841 Char Accuracy: 94.8864 Word Accuracy: 26.5625\n",
      "4m 1s (- 1m 43s) (7 70.00%) Validationloss: 0.3936 Char Accuracy: 87.7497 Word Accuracy: 34.1797\n",
      "4m 35s (- 1m 8s) (8 80.00%) Trainloss: 0.3751 Char Accuracy: 87.1449 Word Accuracy: 29.6875\n",
      "4m 35s (- 1m 8s) (8 80.00%) Validationloss: 0.3747 Char Accuracy: 87.9217 Word Accuracy: 34.2529\n",
      "5m 9s (- 0m 34s) (9 90.00%) Trainloss: 0.1820 Char Accuracy: 95.3125 Word Accuracy: 35.9375\n",
      "5m 9s (- 0m 34s) (9 90.00%) Validationloss: 0.3739 Char Accuracy: 88.0526 Word Accuracy: 34.7656\n",
      "5m 43s (- 0m 0s) (10 100.00%) Trainloss: 0.3857 Char Accuracy: 87.2159 Word Accuracy: 34.3750\n",
      "5m 43s (- 0m 0s) (10 100.00%) Validationloss: 0.3714 Char Accuracy: 88.3079 Word Accuracy: 35.3516\n"
     ]
    }
   ],
   "source": [
    "encoder_model,decoder_model,reshape,encoder_optimizer,decoder_optimizer=train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a3b48f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses_test,epoch_char_accuracy_test,epoch_word_accuracy_test,decoder_predicted=best_acc_model(encoder_model,decoder_model,reshape,\n",
    "                                                encoder_optimizer,decoder_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0738cb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35., 45., 67.,  ...,  0.,  0.,  0.],\n",
       "        [51., 56., 20.,  ...,  0.,  0.,  0.],\n",
       "        [46., 57., 44.,  ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [20., 55., 34.,  ...,  0.,  0.,  0.],\n",
       "        [49., 56., 48.,  ...,  0.,  0.,  0.],\n",
       "        [39., 67., 45.,  ...,  0.,  0.,  0.]], device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "02b4a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_words(input_data, int2char=int2char_hin):\n",
    "    words = []\n",
    "    for i in range(input_data.shape[0]):\n",
    "        word=''\n",
    "        for j in range(input_data.shape[1]):\n",
    "            word=word+int2char[input_data[i,j].item()]\n",
    "        words.append(word)\n",
    "    words_until_eos= []\n",
    "    for word in words:\n",
    "        if '\\n' in word:\n",
    "            word = word.split('\\n')[0]  # Extract the word until '\\n'\n",
    "        words_until_eos.append(word)\n",
    "\n",
    "    return words_until_eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "552283fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=indices_to_words(decoder_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc208942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2718\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_test)):\n",
    "    if len(df_test['English'][i])> 24:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a19bf175",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_test)):\n",
    "    if len(df_test['transliteration_in_hindi'][i])>20:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9391d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(2717, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "039817a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predicted_words'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a61e159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>transliteration_in_hindi</th>\n",
       "      <th>predicted_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thermax</td>\n",
       "      <td>थरमैक्स</td>\n",
       "      <td>थर्मैक्स</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sikhaaega</td>\n",
       "      <td>सिखाएगा</td>\n",
       "      <td>सिखाएगा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learn</td>\n",
       "      <td>लर्न</td>\n",
       "      <td>लीयर्न</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twitters</td>\n",
       "      <td>ट्विटर्स</td>\n",
       "      <td>ट्विटर्स</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tirunelveli</td>\n",
       "      <td>तिरुनेलवेली</td>\n",
       "      <td>तिरुनेलवेली</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>saflata</td>\n",
       "      <td>सफ़लता</td>\n",
       "      <td>सफलता</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>shbana</td>\n",
       "      <td>शबाना</td>\n",
       "      <td>श्बाना</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>khaatootolaa</td>\n",
       "      <td>खातूटोला</td>\n",
       "      <td>खातोतूला</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>shivastava</td>\n",
       "      <td>शिवास्तव</td>\n",
       "      <td>शिवस्तवा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>preranapuree</td>\n",
       "      <td>प्रेरणापुरी</td>\n",
       "      <td>प्रेराणपुरी</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4095 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           English transliteration_in_hindi predicted_words\n",
       "0          thermax                  थरमैक्स        थर्मैक्स\n",
       "1        sikhaaega                  सिखाएगा         सिखाएगा\n",
       "2            learn                     लर्न          लीयर्न\n",
       "3         twitters                 ट्विटर्स        ट्विटर्स\n",
       "4      tirunelveli              तिरुनेलवेली     तिरुनेलवेली\n",
       "...            ...                      ...             ...\n",
       "4090       saflata                   सफ़लता           सफलता\n",
       "4091        shbana                    शबाना          श्बाना\n",
       "4092  khaatootolaa                 खातूटोला        खातोतूला\n",
       "4093    shivastava                 शिवास्तव        शिवस्तवा\n",
       "4094  preranapuree              प्रेरणापुरी     प्रेराणपुरी\n",
       "\n",
       "[4095 rows x 3 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "55d7b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2fda80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
