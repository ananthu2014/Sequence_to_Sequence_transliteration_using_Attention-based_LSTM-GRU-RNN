{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f80d3a-3567-469c-b207-8feceba7ced0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This jupyter notebook contains:\\n1. Data preprocessing\\n2. Transformer Encoder\\n3. Transformer Decoder\\n4. Training\\n5. Evaluation'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This jupyter notebook contains:\n",
    "1. Data preprocessing\n",
    "2. Transformer Encoder\n",
    "3. Transformer Decoder\n",
    "4. Training\n",
    "5. Evaluation'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99b25fe-a9b3-4feb-b9a9-53a355556c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import random\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18940c2f-6098-4db9-875e-40c158d37261",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07989779-01c7-4951-8b8c-b7a51d73e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function to load the data'''\n",
    "def load_data(path,language_names):\n",
    "    df=pd.read_csv(path,header=None)\n",
    "    df.columns=language_names\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e6b9e6-e18e-47dc-9434-abc30f4c7010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51200, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>transliteration_in_hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shastragaar</td>\n",
       "      <td>शस्त्रागार</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bindhya</td>\n",
       "      <td>बिन्द्या</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kirankant</td>\n",
       "      <td>किरणकांत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yagyopaveet</td>\n",
       "      <td>यज्ञोपवीत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ratania</td>\n",
       "      <td>रटानिया</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51195</th>\n",
       "      <td>toned</td>\n",
       "      <td>टोंड</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51196</th>\n",
       "      <td>mutanaazaa</td>\n",
       "      <td>मुतनाज़ा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51197</th>\n",
       "      <td>asahmaton</td>\n",
       "      <td>असहमतों</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51198</th>\n",
       "      <td>sulgaayin</td>\n",
       "      <td>सुलगायीं</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51199</th>\n",
       "      <td>anchuthengu</td>\n",
       "      <td>अंचुतेंगु</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           English transliteration_in_hindi\n",
       "0      shastragaar               शस्त्रागार\n",
       "1          bindhya                 बिन्द्या\n",
       "2        kirankant                 किरणकांत\n",
       "3      yagyopaveet                यज्ञोपवीत\n",
       "4          ratania                  रटानिया\n",
       "...            ...                      ...\n",
       "51195        toned                     टोंड\n",
       "51196   mutanaazaa                 मुतनाज़ा\n",
       "51197    asahmaton                  असहमतों\n",
       "51198    sulgaayin                 सुलगायीं\n",
       "51199  anchuthengu                अंचुतेंगु\n",
       "\n",
       "[51200 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Here,basically,the input is given to the encoder in English language and is transliterated to Hindi\n",
    "by the decoder'''\n",
    "path_train=\"hin_train.csv\"\n",
    "language_names = ['English','transliteration_in_hindi']\n",
    "df_train=load_data(path_train,language_names)\n",
    "print(df_train.shape)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0295381c-c8ef-44dd-a889-50227c3fa9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data: (4096, 2)\n",
      "Test data: (4096, 2)\n"
     ]
    }
   ],
   "source": [
    "path_test=\"hin_test.csv\"\n",
    "path_validation=\"hin_valid.csv\"\n",
    "df_validation=load_data(path_validation,language_names)\n",
    "print('Validation data:',df_validation.shape)\n",
    "df_test=load_data(path_test,language_names)\n",
    "print('Test data:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a300425-3ef3-4143-8431-fdba708fa115",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function for acquiring all the characters of the given data'''\n",
    "def split_words(x):\n",
    "    x=np.array(x)\n",
    "    alpha=['_','\\t','\\n',' '] #pad token, start of word, end of word and unknown tokens respectively\n",
    "    b=[]\n",
    "    for i in range(x.shape[0]):\n",
    "        a=list(x[i])\n",
    "        for j in range(len(a)):\n",
    "            if a[j] not in b:\n",
    "                b.append(a[j])\n",
    "    b=sorted(b)\n",
    "    alpha=alpha+b\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd8106a-23da-46d3-a671-a0f6bcc76452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 30\n",
      "Hindi vocabulary size: 68\n",
      "['_', '\\t', '\\n', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['_', '\\t', '\\n', ' ', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'ळ', 'व', 'श', 'ष', 'स', 'ह', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्']\n"
     ]
    }
   ],
   "source": [
    "'''All the english characters are stored into the list english_vocab and all the hindi characters are\n",
    "stored into the list hindi_vocab'''\n",
    "english_vocab=split_words(df_train['English'])\n",
    "hindi_vocab=split_words(df_train['transliteration_in_hindi'])\n",
    "print('English vocabulary size:', len(english_vocab))\n",
    "print('Hindi vocabulary size:', len(hindi_vocab))\n",
    "print(english_vocab)\n",
    "print(hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e39689b-9116-4002-b77a-63d3d3ca81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Functions to create the vocabulary dictionaries with their corresponding indices'''\n",
    "def int_to_char(vocab):\n",
    "    int2char={} #padding token, start of word, end of word token and unknown token\n",
    "    for i in range(len(vocab)):\n",
    "        int2char[i]=vocab[i]\n",
    "    return int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d068ba44-5050-4672-adb9-f8c557e77e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '_', 1: '\\t', 2: '\\n', 3: ' ', 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 't', 24: 'u', 25: 'v', 26: 'w', 27: 'x', 28: 'y', 29: 'z'}\n"
     ]
    }
   ],
   "source": [
    "int2char_eng=int_to_char(english_vocab)\n",
    "print(int2char_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af101473-30b0-43bc-936b-807aa42b036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_int(int2char):\n",
    "    char2int={ch:ii for ii,ch in int2char.items()}\n",
    "    return char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee7c311b-1e25-43b9-9f8b-8c0261ec8e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_': 0, '\\t': 1, '\\n': 2, ' ': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29}\n"
     ]
    }
   ],
   "source": [
    "char2int_eng=char_to_int(int2char_eng)\n",
    "print(char2int_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda25faf-47bb-41b8-a0e9-3a7ac8516085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '_', 1: '\\t', 2: '\\n', 3: ' ', 4: 'ँ', 5: 'ं', 6: 'ः', 7: 'अ', 8: 'आ', 9: 'इ', 10: 'ई', 11: 'उ', 12: 'ऊ', 13: 'ऋ', 14: 'ए', 15: 'ऐ', 16: 'ऑ', 17: 'ओ', 18: 'औ', 19: 'क', 20: 'ख', 21: 'ग', 22: 'घ', 23: 'ङ', 24: 'च', 25: 'छ', 26: 'ज', 27: 'झ', 28: 'ञ', 29: 'ट', 30: 'ठ', 31: 'ड', 32: 'ढ', 33: 'ण', 34: 'त', 35: 'थ', 36: 'द', 37: 'ध', 38: 'न', 39: 'प', 40: 'फ', 41: 'ब', 42: 'भ', 43: 'म', 44: 'य', 45: 'र', 46: 'ल', 47: 'ळ', 48: 'व', 49: 'श', 50: 'ष', 51: 'स', 52: 'ह', 53: '़', 54: 'ऽ', 55: 'ा', 56: 'ि', 57: 'ी', 58: 'ु', 59: 'ू', 60: 'ृ', 61: 'ॅ', 62: 'े', 63: 'ै', 64: 'ॉ', 65: 'ो', 66: 'ौ', 67: '्'}\n"
     ]
    }
   ],
   "source": [
    "int2char_hin=int_to_char(hindi_vocab)\n",
    "print(int2char_hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bc02904-626f-448d-8904-6110bd906aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_': 0, '\\t': 1, '\\n': 2, ' ': 3, 'ँ': 4, 'ं': 5, 'ः': 6, 'अ': 7, 'आ': 8, 'इ': 9, 'ई': 10, 'उ': 11, 'ऊ': 12, 'ऋ': 13, 'ए': 14, 'ऐ': 15, 'ऑ': 16, 'ओ': 17, 'औ': 18, 'क': 19, 'ख': 20, 'ग': 21, 'घ': 22, 'ङ': 23, 'च': 24, 'छ': 25, 'ज': 26, 'झ': 27, 'ञ': 28, 'ट': 29, 'ठ': 30, 'ड': 31, 'ढ': 32, 'ण': 33, 'त': 34, 'थ': 35, 'द': 36, 'ध': 37, 'न': 38, 'प': 39, 'फ': 40, 'ब': 41, 'भ': 42, 'म': 43, 'य': 44, 'र': 45, 'ल': 46, 'ळ': 47, 'व': 48, 'श': 49, 'ष': 50, 'स': 51, 'ह': 52, '़': 53, 'ऽ': 54, 'ा': 55, 'ि': 56, 'ी': 57, 'ु': 58, 'ू': 59, 'ृ': 60, 'ॅ': 61, 'े': 62, 'ै': 63, 'ॉ': 64, 'ो': 65, 'ौ': 66, '्': 67}\n"
     ]
    }
   ],
   "source": [
    "char2int_hin=char_to_int(int2char_hin)\n",
    "print(char2int_hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62ea6931-8919-4cb1-8dfa-e2f110e8a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Finding the maximum sequence length'''\n",
    "length_eng=[len(i) for i in df_train['English']]\n",
    "length_hin=[len(i) for i in df_train['transliteration_in_hindi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8154e97-4505-4a54-ac9b-256652574cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sequence length of English words is 26\n",
      "The maximum sequence length of transliterated words is 22\n"
     ]
    }
   ],
   "source": [
    "length_eng_max=max(length_eng)+2 #we have to account for the start and end token\n",
    "print(f'The maximum sequence length of English words is {length_eng_max}')\n",
    "length_hin_max=max(length_hin)+2\n",
    "print(f'The maximum sequence length of transliterated words is {length_hin_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e01597ef-66bc-4acf-8162-eecc99d44da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df,english_vocab=english_vocab,hindi_vocab=hindi_vocab,\n",
    "                 length_eng_max=length_eng_max,length_hin_max=length_hin_max,char2int_eng=char2int_eng\n",
    "                 ,char2int_hin=char2int_hin):\n",
    "    \n",
    "    '''removing words of length more than max length'''\n",
    "    df['English'] = df['English'].str.lower()\n",
    "    df['transliteration_in_hindi'] = df['transliteration_in_hindi'].str.lower()\n",
    "    df = df[df['English'].apply(len) <= length_eng_max-2]\n",
    "    df = df[df['transliteration_in_hindi'].apply(len) <= length_hin_max-2]\n",
    "    \n",
    "    '''Adding start and end of word tokens'''\n",
    "    y_og = df['transliteration_in_hindi'].values #The data type of y_og will be numpy array\n",
    "    x_og = df['English'].values\n",
    "    x = '\\t'+x_og+'\\n'\n",
    "    y = '\\t'+y_og+'\\n'\n",
    "    y_do=y_og+'\\n' #This is for the decoder output\n",
    "    unknown=3\n",
    "    pad=0\n",
    "    pad_char='_'\n",
    "    unknown_char=' '\n",
    "    start=1\n",
    "    end=2\n",
    "    \n",
    "    enc_input_data=torch.zeros(len(x),length_eng_max)\n",
    "    dec_input_data=torch.zeros(len(y),length_hin_max)\n",
    "    dec_output_data=torch.zeros(len(y),length_hin_max)\n",
    "    for i, (xx,yy) in enumerate(zip(x,y)):\n",
    "        for j,char in enumerate(xx):\n",
    "            enc_input_data[i,j]=char2int_eng[char]\n",
    "            \n",
    "        #pad character is zero so no need of assigning it again\n",
    "        for j,char in enumerate(yy):\n",
    "            if char in hindi_vocab:\n",
    "                dec_input_data[i,j]=char2int_hin[char]\n",
    "            else:\n",
    "                dec_input_data[i,j]=char2int_hin[unknown_char] #There are chances that unknown char would come in the test data\n",
    "    \n",
    "    for i, (xx,yy) in enumerate(zip(x,y_do)):\n",
    "        for j,char in enumerate(yy):\n",
    "            if char in hindi_vocab:\n",
    "                dec_output_data[i,j]=char2int_hin[char]\n",
    "            else:\n",
    "                dec_input_data[i,j]=char2int_hin[unknown_char]\n",
    "                \n",
    "    return enc_input_data,dec_input_data,dec_output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b47e667-daf1-4d6a-9619-6d93e0220654",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_data,dec_input_data,dec_output_data=process_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8f84c38-e7c6-4642-8c26-e1aa1a068b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51200, 26])\n",
      "torch.Size([51200, 22])\n",
      "torch.Size([51200, 22])\n"
     ]
    }
   ],
   "source": [
    "print(enc_input_data.shape)\n",
    "print(dec_input_data.shape)\n",
    "print(dec_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c5b84d5-fd05-485e-899e-510f71b917da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9c85674-87f1-4db1-a615-5468f8b3a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df,english_vocab=english_vocab,hindi_vocab=hindi_vocab,\n",
    "                 length_eng_max=length_eng_max,length_hin_max=length_hin_max,char2int_eng=char2int_eng\n",
    "                 ,char2int_hin=char2int_hin):\n",
    "    \n",
    "    \n",
    "    '''removing words of length more than max length'''\n",
    "    df = df[df['English'].apply(len) <= length_eng_max-2]\n",
    "    df = df[df['transliteration_in_hindi'].apply(len) <= length_hin_max-2]\n",
    "    '''Adding start and end of word tokens'''\n",
    "    y = df['transliteration_in_hindi'].values\n",
    "    x= df['English'].values\n",
    "    x = '\\t'+x+'\\n'\n",
    "    y = '\\t'+y+'\\n'\n",
    "    \n",
    "    unknown=3\n",
    "    pad=0\n",
    "    pad_char='_'\n",
    "    unknown_char=' '\n",
    "    start=1\n",
    "    end=2\n",
    "    num_english_tokens = len(english_vocab)\n",
    "    num_hindi_tokens = len(hindi_vocab)\n",
    "    \n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(df['English']), length_eng_max, num_english_tokens), dtype=\"float32\")\n",
    "    decoder_input_data = np.zeros(\n",
    "    (len(df['transliteration_in_hindi']), length_hin_max, num_hindi_tokens), dtype=\"float32\")\n",
    "    decoder_output_data = np.zeros(\n",
    "    (len(df['transliteration_in_hindi']), length_hin_max, num_hindi_tokens), dtype=\"float32\")\n",
    "   \n",
    "    for i , (input_text,target_text) in enumerate(zip(x,y)):\n",
    "        for t,char in enumerate(input_text):\n",
    "            encoder_input_data[i,t,char2int_eng[char]]=1\n",
    "        encoder_input_data[i,t+1:,char2int_eng[pad_char]]=1\n",
    "    \n",
    "        for t,char in enumerate(target_text):\n",
    "            if char in hindi_vocab:\n",
    "                decoder_input_data[i,t,char2int_hin[char]]=1\n",
    "            else:\n",
    "                decoder_input_data[i,t,char2int_hin[unknown_char]]=1\n",
    "        decoder_input_data[i,t+1:,char2int_hin[pad_char]]=1\n",
    "    \n",
    "        '''decoder target data is one step ahead of decoder input data by one timestep\n",
    "        and doesnot includes start token'''\n",
    "        for t,char in enumerate(target_text):\n",
    "            if t>0:\n",
    "                if char in hindi_vocab:\n",
    "                    decoder_output_data[i,t-1,char2int_hin[char]]=1\n",
    "                else:\n",
    "                    decoder_output_data[i,t-1,char2int_hin[unknown_char]]=1\n",
    "                \n",
    "        decoder_output_data[i,t:,char2int_hin[pad_char]]=1\n",
    "    \n",
    "    return torch.tensor(encoder_input_data),torch.tensor(decoder_input_data),torch.tensor(decoder_output_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff395a6e-559d-4174-ad48-fae21f5d8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data,decoder_input_data,decoder_output_data=one_hot_encoding(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d094fd8-7114-4c87-8e22-683af1761148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51200, 26, 30])\n",
      "torch.Size([51200, 22, 68])\n",
      "torch.Size([51200, 22, 68])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "727a0901-3c28-4786-a5ef-2095d1267f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_data_test,dec_input_data_test,dec_output_data_test=process_data(df_test)\n",
    "enc_input_data_val,dec_input_data_val,dec_output_data_val=process_data(df_validation)\n",
    "encoder_input_data_test,decoder_input_data_test,decoder_output_data_test=one_hot_encoding(df_test)\n",
    "encoder_input_data_val,decoder_input_data_val,decoder_output_data_val=one_hot_encoding(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74d097ee-cdbc-412e-9e2d-7e3b38e686b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 26, 30])\n",
      "torch.Size([4096, 22, 68])\n",
      "torch.Size([4096, 22, 68])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data_val.shape)\n",
    "print(decoder_input_data_val.shape)\n",
    "print(decoder_output_data_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d367cb8c-69d8-41e5-a8ae-10fc3b113d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 49, 51,  ...,  0,  0,  0],\n",
      "        [ 1, 41, 56,  ...,  0,  0,  0],\n",
      "        [ 1, 19, 56,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 1,  7, 51,  ...,  0,  0,  0],\n",
      "        [ 1, 51, 58,  ...,  0,  0,  0],\n",
      "        [ 1,  7,  5,  ...,  0,  0,  0]])\n",
      "tensor([[ 1., 49., 51.,  ...,  0.,  0.,  0.],\n",
      "        [ 1., 41., 56.,  ...,  0.,  0.,  0.],\n",
      "        [ 1., 19., 56.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  7., 51.,  ...,  0.,  0.,  0.],\n",
      "        [ 1., 51., 58.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  7.,  5.,  ...,  0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(decoder_input_data,dim=-1))\n",
    "print(dec_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84e4b3f5-6b52-4b40-ba66-cb9e61419751",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_data=enc_input_data.long()\n",
    "dec_input_data=dec_input_data.long()\n",
    "enc_input_data_test=enc_input_data_test.long()\n",
    "dec_input_data_test=dec_input_data_test.long()\n",
    "enc_input_data_val=enc_input_data_val.long()\n",
    "dec_input_data_val=dec_input_data_val.long()\n",
    "encoder_input_data=encoder_input_data.long()\n",
    "decoder_input_data=decoder_input_data.long()\n",
    "decoder_output_data=decoder_output_data.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "662712d1-bf2a-4250-8517-2dcef1dbfafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 26, 44,  ...,  0,  0,  0],\n",
       "        [ 1, 41, 26,  ...,  0,  0,  0],\n",
       "        [ 1, 51,  5,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 1, 14, 19,  ...,  0,  0,  0],\n",
       "        [ 1, 41, 67,  ...,  0,  0,  0],\n",
       "        [ 1, 21, 65,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_input_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8846549e-4181-4bbe-8106-cefcc63984c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26., 44., 51.,  ...,  0.,  0.,  0.],\n",
       "        [41., 26., 55.,  ...,  0.,  0.,  0.],\n",
       "        [51.,  5., 22.,  ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [14., 19., 55.,  ...,  0.,  0.,  0.],\n",
       "        [41., 67., 46.,  ...,  0.,  0.,  0.],\n",
       "        [21., 65., 48.,  ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e93141f2-822c-4295-a033-45753c8ab51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere, the implementation is based on the paper: Attention is all you need\\nThe original base attention transformer has the following structure:\\n\\nMain dimension of model-embeddings (d_model): 512\\nNumber of attention heads: 8\\nNumber of encoder layers: 6\\nNumber of decoder layers: 6\\nHidden dimension of feed-forward layers: 2048\\nDropout probability: 0.1\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here, the implementation is based on the paper: Attention is all you need\n",
    "The original base attention transformer has the following structure:\n",
    "\n",
    "Main dimension of model-embeddings (d_model): 512\n",
    "Number of attention heads: 8\n",
    "Number of encoder layers: 6\n",
    "Number of decoder layers: 6\n",
    "Hidden dimension of feed-forward layers: 2048\n",
    "Dropout probability: 0.1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "858539cc-9d23-4360-b187-ad99980dfe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_generator(x, heads, seq_len_dec = None, type = 'encoder_selfattention'):\n",
    "\n",
    "    '''\n",
    "    Here, the output will be of the format (bs, heads, seq_len, seq_len), which can be used for masking in the multihead attention block\n",
    "    before applying softmax function\n",
    "    types : Encoder self attention(bs, seq_len, enc_seqlen, enc_seqlen)\n",
    "            Decoder self attention(bs, seq_len, dec_seqlen, dec_seqlen)\n",
    "            Decoder cross attention(bs, seq_len, dec_seqlen, enc_seqlen)\n",
    "    '''\n",
    "    \n",
    "    if type == 'encoder_selfattention':\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        pad_idx = 0\n",
    "        mask = (x == pad_idx)\n",
    "        '''\n",
    "        Expand the mask for attention (batch_size, num_heads, seq_len, seq_len); after doing the operation q.kT the shape will be this\n",
    "        '''\n",
    "        mask = mask.unsqueeze(1).unsqueeze(2).expand(batch_size, heads, seq_len, seq_len).detach().to(device)\n",
    "        \n",
    "        \n",
    "    elif type == 'decoder_selfattention':\n",
    "        '''Here, this masking is used in decoder to avoid the present tokens seeing the future ones'''\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        mask = torch.triu(torch.ones(seq_len,seq_len)*float('-inf'), diagonal = 1).bool().expand(batch_size, heads, seq_len, seq_len).detach().to(device)\n",
    "\n",
    "    elif type  == 'decoder_crossattention':\n",
    "        '''Here, we use the padding_mask of encoder and the sequence length should be that of encoder\n",
    "        output : (batch size, heads, decoder sequence length, encoder sequence langth'''\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len_enc = x.shape[1]\n",
    "        pad_idx = 0\n",
    "        mask = (x == pad_idx)\n",
    "        mask = mask.unsqueeze(1).unsqueeze(2).expand(batch_size, heads, seq_len_dec, seq_len_enc).detach().to(device)\n",
    "        \n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f7b4ddd-cffc-4efe-b464-21e85ddbf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding(seq_length, d_model):\n",
    "    '''\n",
    "    \n",
    "    shape : (seq_len, d_model)\n",
    "    Gradient won't flow through positional embedding\n",
    "    '''\n",
    "    pe = torch.zeros((seq_length, d_model), requires_grad = False)\n",
    "    for pos in range(seq_length):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "            pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "    pe = pe.unsqueeze(0)\n",
    "\n",
    "    return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f452d5a-dd76-4547-9a0e-96d515135c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiheadattention(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model, h, dropout):\n",
    "        \n",
    "        super(multiheadattention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = h\n",
    "        self.d_k = d_model//h\n",
    "        self.q = nn.Linear(d_model,d_model, bias = False)\n",
    "        self.k = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.v = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.outputprojectionlayer = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self,w, mask):\n",
    "        '''x: Input data after performing positional and word embeddings and adding both\n",
    "           att_scores : output of same shape\n",
    "           d_model shpuld be divisible by number of heads\n",
    "        '''\n",
    "        q = self.q(w[0])\n",
    "        k = self.k(w[1])\n",
    "        v = self.v(w[2])\n",
    "\n",
    "        \n",
    "        bs = q.shape[0]\n",
    "        seq_len1 = w[0].shape[1]\n",
    "        seq_len2 = w[1].shape[1]\n",
    "        q = q.view(bs, seq_len1, self.heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(bs, seq_len2, self.heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(bs, seq_len2, self.heads, self.d_k).transpose(1, 2)\n",
    "       \n",
    "        \n",
    "\n",
    "        att_scores = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(self.d_k)\n",
    "      \n",
    "        \n",
    "        if mask is not None:\n",
    "            att_scores = att_scores.masked_fill(mask,float('-inf'))\n",
    "       \n",
    "        att_scores = F.softmax(att_scores, dim = -1)\n",
    "        att_scores = self.dropout(att_scores)\n",
    "        att = torch.matmul(att_scores, v)\n",
    "        \n",
    "      \n",
    "        \n",
    "        #Concatenating the heads to form the shape of (bs, seq_len, d_model)\n",
    "        att = att.transpose(1, 2).contiguous().view(bs, seq_len1, self.d_model)\n",
    "        \n",
    "        output = self.outputprojectionlayer(att)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb122e3e-8967-424f-a337-1c1561e7b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pointwiseffnn(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_hidden):\n",
    "        super(pointwiseffnn, self).__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_model = d_model\n",
    "        self.linearlayer1 = nn.Linear(d_model, d_hidden)\n",
    "        self.linearlayer2 = nn.Linear(d_hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        This is the pointwise feedforward layer after attention block, which consists of 2 linear layers, a relu\n",
    "        '''\n",
    "        x = self.linearlayer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linearlayer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18a57b7e-b199-4b7e-813c-26114493c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoderlayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, h, d_hidden, dropout):\n",
    "        '''\n",
    "        d_model : Embedding dimension(512)\n",
    "        h : Number of heads(8)\n",
    "        N : number of attention layers(6)\n",
    "        d_k : d_model/h\n",
    "        seq : Maximum sequence length of English(26)\n",
    "        vocab_size : Number of English characters in the corpus(30)\n",
    "        epsilon : used for layer normalization (10e-5)\n",
    "        '''\n",
    "        super(Encoderlayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = h\n",
    "        self.d_k = d_model//h\n",
    "        self.d_hidden = d_hidden\n",
    "        self.mha = multiheadattention(d_model, h, dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(self.d_model)\n",
    "        self.pffnn = pointwiseffnn(d_model, d_hidden)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layernorm2 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x, sam):\n",
    "            \n",
    "        x_mha = self.mha([x, x, x], sam)\n",
    "        x_mha = self.dropout1(x_mha)\n",
    "        x_out1 = self.layernorm1(x + x_mha)\n",
    "        \n",
    "        x_pffnn = self.pffnn(x_out1)\n",
    "        x_pffnn = self.dropout2(x_pffnn)\n",
    "        x_out2 = self.layernorm2(x_out1 + x_pffnn)\n",
    "\n",
    "        return x_out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "03eb9903-0100-474a-8163-126cc0d11a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function can be used to create multiple encoder blocks with different weights, here we need 6 encoder blocks'''\n",
    "def multipleblocks(block, N):\n",
    "    \n",
    "    return nn.ModuleList([copy.deepcopy(block) for i in range(N)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c879467b-34d2-48e0-81a5-776b4f566dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, h, vocab_size_encoder, N, d_hidden, dropout):\n",
    "        '''\n",
    "        d_model : Embedding dimension(512)\n",
    "        h : Number of heads(8)\n",
    "        N : number of attention layers(6)\n",
    "        d_k : d_model/h\n",
    "        seq : Maximum sequence length of English(26)\n",
    "        vocab_size : Number of English characters in the corpus(30)\n",
    "        epsilon : used for layer normalization (10e-5)\n",
    "        pe_encoder : Positional embedding of Encoder input\n",
    "        \n",
    "        '''\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = h\n",
    "        self.d_k = d_model//h\n",
    "        self.vocab_size_encoder = vocab_size_encoder\n",
    "        self.N = N\n",
    "        self.d_hidden = d_hidden\n",
    "        self.embedding = nn.Embedding(vocab_size_encoder, d_model, padding_idx = 0)\n",
    "        self.layers = multipleblocks(Encoderlayer(d_model, h, d_hidden, dropout), N)\n",
    "        \n",
    "        assert self.d_model % self.heads == 0\n",
    "\n",
    "\n",
    "    def forward(self, x, sam, pe):\n",
    "        '''\n",
    "        x : Encoder input embedded\n",
    "        sam : self attention padding mask\n",
    "        '''\n",
    "        x_wv = self.embedding(x)\n",
    "        x = x_wv + pe\n",
    "\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, sam)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "02cf5c1a-92d4-47d4-8353-fea236de6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderlayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, h, d_hidden, dropout):\n",
    "       \n",
    "        '''\n",
    "        d_model : Embedding dimension(512)\n",
    "        h : Number of heads(8)\n",
    "        N : number of attention layers(6)\n",
    "        d_k : d_model/h\n",
    "        seq : Maximum sequence length of English(26)\n",
    "        vocab_size : Number of English characters in the corpus(30)\n",
    "        epsilon : used for layer normalization (10e-5)\n",
    "        '''\n",
    "        super(Decoderlayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = h\n",
    "        self.d_hidden = d_hidden\n",
    "        self.mha_self = multiheadattention(d_model, h, dropout)\n",
    "        self.mha_cross = multiheadattention(d_model, h, dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(self.d_model)\n",
    "        self.pffnn = pointwiseffnn(d_model, d_hidden)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.layernorm2 = nn.LayerNorm(self.d_model)\n",
    "        self.layernorm3 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, sam, cam):\n",
    "        '''\n",
    "        x : embedded decoder input\n",
    "        y : encoder output\n",
    "        sam : Self Attention mask\n",
    "        cam : cross attention mask\n",
    "        \n",
    "        '''\n",
    "            \n",
    "        x_mha_self = self.mha_self([x, x, x], sam)\n",
    "        x_mha_self = self.dropout1(x_mha_self)\n",
    "        x_out1 = self.layernorm1(x + x_mha_self)\n",
    "        #print('after self',x_out1.shape)\n",
    "        \n",
    "        x_mha_cross = self.mha_cross([x, y, y], cam)\n",
    "        x_mha_cross = self.dropout2(x_mha_cross)\n",
    "        x_out2 = self.layernorm2(x_out1 + x_mha_cross)\n",
    "        \n",
    "        x_pffnn = self.pffnn(x_out2)\n",
    "        x_pffnn = self.dropout2(x_pffnn)\n",
    "        x_out3 = self.layernorm3(x_out2 + x_pffnn)\n",
    "\n",
    "        return x_out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5940497-92ca-462d-a052-b5b2dc14471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, h, vocab_size_decoder, N, d_hidden, dropout):\n",
    "        '''\n",
    "        d_model : Embedding dimension(512)\n",
    "        h : Number of heads(8)\n",
    "        N : number of attention layers(6)\n",
    "        d_k : d_model/h\n",
    "        seq : Maximum sequence length of English(26)\n",
    "        vocab_size : Number of English characters in the corpus(30)\n",
    "        epsilon : used for layer normalization (10e-5)\n",
    "        pe_decoder : Positional embedding of Decoder input\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = h\n",
    "        self.d_k = d_model//h\n",
    "        self.vocab_size_decoder = vocab_size_decoder\n",
    "        self.N = N\n",
    "        self.d_hidden = d_hidden\n",
    "        self.embedding = nn.Embedding(vocab_size_decoder, d_model, padding_idx = 0)\n",
    "        self.layers = multipleblocks(Decoderlayer(d_model, h, d_hidden, dropout), N)\n",
    "        \n",
    "        assert self.d_model % self.heads == 0\n",
    "\n",
    "\n",
    "    def forward(self, x, y, sam, cam, pe):\n",
    "        '''\n",
    "        x : decoder input\n",
    "        y : encoder output\n",
    "        sam : Self Attention Mask\n",
    "        cam : Cross Attention Mask\n",
    "        pe : positional embedding of decoder tokens\n",
    "        \n",
    "        '''\n",
    "\n",
    "        x_wv = self.embedding(x)\n",
    "        x = x_wv + pe\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, y, sam, cam)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b38476b8-8aac-40b1-b9dd-effffdbae983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads, vocab_size_encoder, vocab_size_decoder, N, d_hidden, dropout):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = d_model//heads\n",
    "        self.N = N\n",
    "        self.d_hidden = d_hidden\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size_encoder = vocab_size_encoder\n",
    "        self.vocab_size_decoder = vocab_size_decoder\n",
    "        self.encoder = Encoder(d_model, heads, vocab_size_encoder, N, d_hidden, dropout)\n",
    "        self.decoder = Decoder(d_model, heads, vocab_size_decoder, N, d_hidden, dropout)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, vocab_size_decoder)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, encoder_sam, decoder_sam, decoder_cam, enc_pe, dec_pe):\n",
    "        '''\n",
    "        \n",
    "        x : encoder input data\n",
    "        y : decoder input data\n",
    "        encoding : encoder output data\n",
    "        decoding : decoder output data\n",
    "        No need to find the softmax/probability values as it will be already taken care by Cross Entropy loss function.\n",
    "        '''\n",
    "       \n",
    "        encoding = self.encoder(x, encoder_sam, enc_pe)\n",
    "        decoding = self.decoder(y, encoding, decoder_sam, decoder_cam, dec_pe)\n",
    "\n",
    "        logits = self.linear(decoding)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7ff681a5-e0b8-4fe7-b29a-ff5d86d07e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, filepath):\n",
    "    \"\"\"\n",
    "    Save the model state, optimizer state, epoch, and training history to a file.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to be saved.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used in training.\n",
    "        epoch (int): The current epoch number.\n",
    "        loss_history (list): List of loss values per epoch.\n",
    "        accuracy_history (list): List of accuracy values per epoch.\n",
    "        char_history (list): List of character accuracy values per epoch.\n",
    "        filepath (str): The path to the file where the state will be saved.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(state, filepath)\n",
    "    print(f\"Model saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3fe361b1-c224-4935-bf4f-c2ca95a87da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer, filepath):\n",
    "    \"\"\"\n",
    "    Load the model state, optimizer state, epoch, and training history from a file.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to be loaded.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used in training.\n",
    "        filepath (str): The path to the file from where the state will be loaded.\n",
    "\n",
    "    Returns:\n",
    "        int: The epoch number from which to resume training.\n",
    "        list: List of loss values per epoch.\n",
    "        list: List of accuracy values per epoch.\n",
    "        list: List of character accuracy values per epoch.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return epoch, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d3c40932-985b-4adc-8c84-690eae793565",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set the value of hyper parameters before training\n",
    "'''\n",
    "d_model = 512\n",
    "d_hidden = 2048\n",
    "vocab_size_encoder = 30\n",
    "vocab_size_decoder = 68\n",
    "N = 6\n",
    "heads = 8\n",
    "dropout = 0.1\n",
    "seq_len_encoder = 26\n",
    "seq_len_decoder = 22\n",
    "learning_rate = 0.0001\n",
    "epsilon = 1e-8\n",
    "betas = (0.9, 0.98)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "33561f1e-6e6d-4bf5-ab97-4558108ff5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating an instance of transformer, optimizer, loss function\n",
    "'''\n",
    "model = Transformer(d_model, heads, vocab_size_encoder, vocab_size_decoder, N, d_hidden, dropout)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=betas, eps=epsilon, weight_decay=0.005)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "12a641dc-bb87-41df-b698-e8883a90e4b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(30, 512, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0): Encoderlayer(\n",
      "        (mha): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Encoderlayer(\n",
      "        (mha): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): Encoderlayer(\n",
      "        (mha): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): Encoderlayer(\n",
      "        (mha): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): Encoderlayer(\n",
      "        (mha): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): Encoderlayer(\n",
      "        (mha): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(68, 512, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0): Decoderlayer(\n",
      "        (mha_self): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha_cross): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Decoderlayer(\n",
      "        (mha_self): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha_cross): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): Decoderlayer(\n",
      "        (mha_self): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha_cross): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): Decoderlayer(\n",
      "        (mha_self): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha_cross): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): Decoderlayer(\n",
      "        (mha_self): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha_cross): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): Decoderlayer(\n",
      "        (mha_self): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (mha_cross): multiheadattention(\n",
      "          (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (outputprojectionlayer): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (pffnn): pointwiseffnn(\n",
      "          (linearlayer1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linearlayer2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=68, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assigning the transformer object into multiple GPUs\n",
    "'''\n",
    "model = model.to(device)\n",
    "# model = nn.DataParallel(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b212468c-6b04-4fd9-842a-cc14869f3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(x,y,z,batch_size,device=device):\n",
    "    \n",
    "    x=x.to(device)\n",
    "    y=y.to(device)\n",
    "    z=z.to(device)\n",
    "    combined=TensorDataset(x,y,z)\n",
    "    loader=DataLoader(combined,batch_size=batch_size,shuffle=False,drop_last=True)#required in test data\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "add5d14a-dc8e-48c5-ad78-62462235f5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 26])\n",
      "torch.Size([8, 22])\n",
      "torch.Size([8, 22])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Creating data loader\n",
    "'''\n",
    "batch_size = 8\n",
    "train_loader = data_loader(enc_input_data, dec_input_data, dec_output_data, batch_size)\n",
    "val_loader = data_loader(enc_input_data_val, dec_input_data_val, dec_output_data_val, batch_size)\n",
    "for i, (x, y, z) in enumerate(train_loader):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(z.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "56df171d-319a-4109-ab6d-3dcddec655e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_accuracy(dec_predicted_data, dec_output_data):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        match = (dec_predicted_data == dec_output_data).all(dim=1)\n",
    "        true_words = match.sum().item()\n",
    "        batch_size = dec_predicted_data.shape[0]\n",
    "    \n",
    "    accuracy = (true_words / batch_size) * 100 \n",
    "    return accuracy  #Averaged over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2e4b0796-7536-46c1-a789-6b8caf1dc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_char_accuracy(decoder_predicted_data, decoder_output_data):\n",
    "   \n",
    "    batch_size, seq_length = decoder_predicted_data.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct_count = (decoder_predicted_data == decoder_output_data).sum().item() \n",
    "        return (correct_count / (seq_length * batch_size))*100 #Averaged over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "71675722-6693-4550-ad61-2673f677b73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 26, 512])\n",
      "torch.Size([1, 22, 512])\n"
     ]
    }
   ],
   "source": [
    "enc_pe = positional_embedding(seq_len_encoder, d_model).to(device)\n",
    "dec_pe = positional_embedding(seq_len_decoder, d_model).to(device)\n",
    "print(enc_pe.shape)\n",
    "print(dec_pe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a3d9b445-655d-4a10-8298-5682f67f8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, heads, batch_size, d_model, enc_pe, dec_pe, total_samples, device):\n",
    "    '''\n",
    "    Perform decoder predictions autoregressively\n",
    "    '''\n",
    "    model.eval()\n",
    "    word_acc_test = 0.0\n",
    "    char_acc_test = 0.0\n",
    "    predictions = torch.zeros((total_samples, dec_pe.shape[1]+1), device=device)\n",
    "    predictions[:, 0] = 1.0  # Assuming 1.0 is the index for <SOS>\n",
    "\n",
    "    word_acc= 0.0\n",
    "    char_acc = 0.0\n",
    "\n",
    "    for i, (x, y, z) in enumerate(tqdm(test_loader)):\n",
    "        x, y, z = x.to(device), y.to(device), z.to(device)\n",
    "        esm = mask_generator(x, heads, type='encoder_selfattention').to(device)\n",
    "        encoding = model.encoder(x, esm, enc_pe).to(device)\n",
    "\n",
    "        for j in range(1, predictions.shape[1]):\n",
    "            \n",
    "            dcm = mask_generator(x, heads, j, type='decoder_crossattention').to(device)\n",
    "            inp = predictions[i * batch_size:(i + 1) * batch_size, :j].to(device)\n",
    "            preds = model.decoder(inp.long(), encoding, None, None, dec_pe[:, :j])\n",
    "\n",
    "            preds = model.linear(preds)  # Assuming the linear layer is the output layer\n",
    "            last_token = preds[:, -1, :]  # Take the last token's predictions\n",
    "            last_token = F.softmax(last_token, dim=-1)\n",
    "            last_token = torch.argmax(last_token, dim=-1)\n",
    "            predictions[i * batch_size:(i + 1) * batch_size, j] = last_token\n",
    "        \n",
    "        \n",
    "        word_acc_val = calculate_word_accuracy(predictions[i*batch_size:(i+1)*batch_size, 1:], z)\n",
    "        char_acc_val = calculate_char_accuracy(predictions[i*batch_size:(i+1)*batch_size, 1:], z)\n",
    "        word_acc += word_acc_val\n",
    "        char_acc += char_acc_val\n",
    "    word_acc = word_acc/ len(test_loader)\n",
    "    char_acc = char_acc/ len(test_loader)\n",
    "    return predictions[:,1:], word_acc, char_acc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "943bbc16-980f-4163-ab47-5a118b7bd922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, val_loader, epochs, heads, batch_size, d_model, \n",
    "          enc_pe, dec_pe, device):\n",
    "\n",
    "    if os.path.exists('model_checkpoint.pth'):\n",
    "        _, model = load_model(model, optimizer, 'model_checkpoint.pth')\n",
    "        \n",
    "    loss_history = []\n",
    "    train_word_acc_history = []\n",
    "    train_char_acc_history = []\n",
    "    val_word_acc_history = []\n",
    "    val_char_acc_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_word_acc_train = 0.0\n",
    "        epoch_char_acc_train = 0.0\n",
    "        \n",
    "        for i, (x, y, z) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "            x, y, z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "            esm = mask_generator(x, heads, type='encoder_selfattention').to(device)\n",
    "            dsm = mask_generator(y, heads, type='decoder_selfattention').to(device)\n",
    "            dcm = mask_generator(x, heads, y.shape[1], type='decoder_crossattention').to(device)\n",
    "\n",
    "            preds = model(x, y, esm, dsm, dcm, enc_pe, dec_pe)\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(preds.contiguous().view(-1, preds.shape[-1]), z.contiguous().view(-1).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(F.softmax(preds, dim=-1), dim=-1)\n",
    "\n",
    "            word_acc_train = calculate_word_accuracy(preds, z)\n",
    "            char_acc_train = calculate_char_accuracy(preds, z)\n",
    "         \n",
    "            epoch_word_acc_train += word_acc_train\n",
    "            epoch_char_acc_train += char_acc_train\n",
    "\n",
    "        loss_history.append(epoch_loss / len(train_loader)) #Average batch loss\n",
    "        train_word_acc_history.append(epoch_word_acc_train / len(train_loader))\n",
    "        train_char_acc_history.append(epoch_char_acc_train / len(train_loader))\n",
    "\n",
    "        '''Validation'''\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            predictions, epoch_word_acc_val, epoch_char_acc_val = test(model, val_loader, heads, batch_size, \n",
    "                                                               d_model, enc_pe, dec_pe, total_samples = 4096, device = device)\n",
    "\n",
    "        val_word_acc_history.append(epoch_word_acc_val)\n",
    "        val_char_acc_history.append(epoch_char_acc_val)\n",
    "            \n",
    "        print(f\"Epoch: {epoch+1}, Loss: {epoch_loss / (i + 1):.4f}, Train Word Accuracy: {epoch_word_acc_train / (i + 1):.4f}, Train Char Accuracy: {epoch_char_acc_train / (i + 1):.4f}, Val Word Accuracy: {epoch_word_acc_val:.4f}, Val Char Accuracy: {epoch_char_acc_val:.4f}\")\n",
    "        \n",
    "        save_model(model, optimizer, epoch, 'model_checkpoint.pth')\n",
    "    \n",
    "    return loss_history, train_word_acc_history, train_char_acc_history, val_word_acc_history, val_char_acc_history, predictions, preds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "96ace2e6-c345-412e-8724-68008d0ac5e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████| 6400/6400 [03:55<00:00, 27.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:48<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.3241, Train Word Accuracy: 15.4004, Train Char Accuracy: 89.6737, Val Word Accuracy: 9.0576, Val Char Accuracy: 76.1819\n",
      "Model saved to model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████████████████████████████████████████████████████████| 6400/6400 [03:50<00:00, 27.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:50<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.1775, Train Word Accuracy: 26.7422, Train Char Accuracy: 93.8968, Val Word Accuracy: 9.9854, Val Char Accuracy: 79.5521\n",
      "Model saved to model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████████████████████████████████████████████████████████████| 6400/6400 [03:49<00:00, 27.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:48<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.1546, Train Word Accuracy: 30.8516, Train Char Accuracy: 94.6597, Val Word Accuracy: 15.9424, Val Char Accuracy: 81.5063\n",
      "Model saved to model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|███████████████████████████████████████████████████████████████████| 6400/6400 [03:47<00:00, 28.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:48<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.1428, Train Word Accuracy: 33.8770, Train Char Accuracy: 95.0737, Val Word Accuracy: 13.9893, Val Char Accuracy: 79.5443\n",
      "Model saved to model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████████████████████████████████████████████████████████████| 6400/6400 [03:50<00:00, 27.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:51<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.1343, Train Word Accuracy: 35.8438, Train Char Accuracy: 95.3427, Val Word Accuracy: 16.3574, Val Char Accuracy: 81.3310\n",
      "Model saved to model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|███████████████████████████████████████████████████████████████████| 6400/6400 [03:49<00:00, 27.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:49<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.1287, Train Word Accuracy: 37.5020, Train Char Accuracy: 95.5445, Val Word Accuracy: 16.3574, Val Char Accuracy: 82.0634\n",
      "Model saved to model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████████████████████████████████████████████| 6400/6400 [03:50<00:00, 27.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:49<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.1235, Train Word Accuracy: 38.9805, Train Char Accuracy: 95.7157, Val Word Accuracy: 17.2852, Val Char Accuracy: 81.8703\n",
      "Model saved to model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|███████████████████████████████████████████████████████████████████| 6400/6400 [03:50<00:00, 27.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:49<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.1194, Train Word Accuracy: 39.9648, Train Char Accuracy: 95.8481, Val Word Accuracy: 13.4766, Val Char Accuracy: 81.3354\n",
      "Model saved to model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|███████████████████████████████████████████████████████████████████| 6400/6400 [03:50<00:00, 27.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:51<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.1156, Train Word Accuracy: 41.5312, Train Char Accuracy: 95.9930, Val Word Accuracy: 11.8408, Val Char Accuracy: 81.1668\n",
      "Model saved to model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████████████████████████████████████████████████████████████| 6400/6400 [03:51<00:00, 27.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.1124, Train Word Accuracy: 42.3457, Train Char Accuracy: 96.0977, Val Word Accuracy: 15.1855, Val Char Accuracy: 82.9790\n",
      "Model saved to model_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    " loss_history, train_word_acc_history, train_char_acc_history, val_word_acc_history, val_char_acc_history, test_predictions, preds = train(model, \n",
    "                optimizer, loss, train_loader, val_loader, epochs, heads, batch_size, d_model, enc_pe, dec_pe, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "19ef148c-065e-4289-ad98-2fa29bbf9369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26., 63., 51., 55., 48., 55., 46.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [41., 26., 55., 10.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [51.,  5., 22., 30., 38., 55.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [52., 55., 10., 48., 55., 38.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [38., 57., 46., 21., 56., 31., 53., 57.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], device='cuda:0')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5fbd7250-127d-42e5-acb3-d34bc19d8e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49., 51., 67., 34., 67., 45., 55., 21., 55., 45.,  2.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [41., 56., 38., 67., 36., 67., 44., 55.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [19., 56., 45., 33., 19., 55.,  5., 34.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [44., 26., 67., 28., 65., 39., 48., 57., 34.,  2.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [45., 29., 55., 38., 56., 44., 55.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f3137-aea9-4fea-a720-1a6a6565b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4c12a614-d8f2-4c90-b04d-3f82e2cfa07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "  _, model = load_model(model, optimizer, 'model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "717b9213-7661-42f1-99af-03b7755dc0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 512/512 [00:42<00:00, 12.07it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions, epoch_word_acc_val, epoch_char_acc_val = test(model, val_loader, heads, batch_size, \n",
    "                                                               d_model, enc_pe, dec_pe, total_samples = 4096, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "08c48061-5947-494e-9739-08768b698b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.80078125"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_word_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "dc66d0c1-079d-4abb-b421-8033cde7c893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.26899857954547"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_char_acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3742e3-7899-4b8d-b384-43b2d5a51d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
